{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4bbc2b5-977d-46f0-bd8c-a2828ab8c011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from openai_client import client, call_openai_api\n",
    "from load_configs import openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cf339e-7eb5-46f6-a6e5-322ec6d3de29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7196e07b-ae11-4022-b010-7f33a238433f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': 'completion', 'parsed': 'noitelpmoc'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "\n",
    "runnable = RunnableParallel(\n",
    "    origin=RunnablePassthrough(),\n",
    "    modified=lambda x: x+1\n",
    ")\n",
    "\n",
    "runnable.invoke(1) # {'origin': 1, 'modified': 2}\n",
    "\n",
    "\n",
    "def fake_llm(prompt: str) -> str: # Fake LLM for the example\n",
    "    return \"completion\"\n",
    "\n",
    "chain = RunnableLambda(fake_llm) | {\n",
    "    'original': RunnablePassthrough(), # Original LLM output\n",
    "    'parsed': lambda text: text[::-1] # Parsing logic\n",
    "}\n",
    "\n",
    "chain.invoke('hello') # {'original': 'completion', 'parsed': 'noitelpmoc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef75a9db-4882-4232-bdfe-46bb0f328db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm1': 'completion', 'llm2': 'completion', 'total_chars': 20}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fake_llm(prompt: str) -> str: # Fake LLM for the example\n",
    "    return \"completion\"\n",
    "\n",
    "runnable = {\n",
    "    'llm1':  fake_llm,\n",
    "    'llm2':  fake_llm,\n",
    "} | RunnablePassthrough.assign(\n",
    "    total_chars=lambda inputs: len(inputs['llm1'] + inputs['llm2'])\n",
    ")\n",
    "\n",
    "runnable.invoke('hello')\n",
    "# {'llm1': 'completion', 'llm2': 'completion', 'total_chars': 20}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff0db0e-790b-4341-af35-a54d2cdba21a",
   "metadata": {},
   "source": [
    "# oo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0239d747-198b-40d1-8d17-26dbf329caae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/country_eng.csv', '../data/custom_2020.csv', '../data/Mobile-Price-Prediction-cleaned_data.csv']\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(\"../data\")\n",
    "os.path.exists(data_path)\n",
    "csv_files = glob(f\"{data_path}/*.csv\")\n",
    "print(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0afb098a-6148-4a93-9778-325d14f4c4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      232\n",
       "Country         232\n",
       "Country_name    232\n",
       "Area            232\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfile = csv_files[0]\n",
    "df_small = pd.read_csv(sfile)\n",
    "df_small.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2be6064-683a-411f-8a16-5149a692e01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "exp_imp    3299804\n",
       "Year       3299804\n",
       "month      3299804\n",
       "ym         3299804\n",
       "Country    3299804\n",
       "Custom     3299804\n",
       "hs2        3299804\n",
       "hs4        3299804\n",
       "hs6        3299804\n",
       "hs9        3299804\n",
       "Q1         3299804\n",
       "Q2         3299804\n",
       "Value      3299804\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lfile = csv_files[1]\n",
    "df_large = pd.read_csv(lfile)\n",
    "df_large.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64f4b538-b417-4f2a-ae1b-cbc855e90977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "llm = ChatOpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    # model=\"gpt-4o-mini-2024-07-18\",\n",
    "    # model=\"gpt-4o\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43d06a85-293e-4a1e-bc85-dae000b0b0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_csv_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00e00be6-756a-4c1c-8205-731e6cc52650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/country_eng.csv'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9e4d01a-a28f-4bea-a9ba-962846850d18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/myDev/llmapps/venv/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': 'len(df)'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m232\u001b[0m\u001b[32;1m\u001b[1;3mThere are 232 rows in the dataframe `df`.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are 232 rows in the dataframe `df`.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = create_csv_agent(\n",
    "    llm,\n",
    "    sfile,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "    allow_dangerous_code=True,\n",
    "    verbose=True\n",
    ")\n",
    "agent.run(\"how many rows are there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0f5ac6b-25f4-4bce-8179-43e64e1354e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To determine the number of rows in the dataframe `df`, I can use the `shape` attribute, which returns a tuple where the first element is the number of rows. \n",
      "Action: python_repl_ast\n",
      "Action Input: df.shape\u001b[0m\u001b[36;1m\u001b[1;3m(232, 4)\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: There are 232 rows in the dataframe `df`.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are 232 rows in the dataframe `df`.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = create_csv_agent(\n",
    "    llm,\n",
    "    sfile,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    allow_dangerous_code=True,\n",
    "    verbose=True\n",
    ")\n",
    "agent.run(\"how many rows are there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a9b65f5-56e9-4f55-8d9b-e5a5a5813eec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': 'len(df)'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m232\u001b[0m\u001b[32;1m\u001b[1;3mThere are 232 rows in the dataframe.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "There are 232 rows in the dataframe.\n"
     ]
    }
   ],
   "source": [
    "agent = create_csv_agent(\n",
    "    llm,\n",
    "    sfile,\n",
    "    agent_type=\"tool-calling\", #AgentType.OPENAI_FUNCTIONS,\n",
    "    allow_dangerous_code=True,\n",
    "    verbose=True,\n",
    "    # return_intermediate_steps=True,\n",
    "    # return_direct=True,\n",
    "    # response_format='content_and_artifact'\n",
    ")\n",
    "res = agent.run(input=\"how many rows are there?\", \n",
    "                # return_direct=True,\n",
    "    # response_format='content_and_artifact'\n",
    "                # return_intermediate_steps=True,\n",
    "               )\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc00eb63-aa6c-4498-ad2a-27338ba222a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c497872e-d96e-4b3a-ae01-028c72a436c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typing.Dict[str, typing.Any]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agent.get_allowed_tools()\n",
    "agent.OutputType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "587de43a-3342-48c6-abf3-610c677089cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.input_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c0a9729-9c9a-45da-b92c-d15d5c7ce233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.output_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "241f6df9-ed79-40e8-bc21-f86d8bd715ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.return_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5883e30-42e8-4014-9154-f84ef96ace08",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.tools[0].return_direct = True\n",
    "agent.tools[0].response_format='content'#'content_and_artifact'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef610cfe-8540-438f-9bc4-0a3142d18c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': 'len(df)'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m232\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "232\n"
     ]
    }
   ],
   "source": [
    "res = agent.run(input=\"how many rows are there?\",)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "347183fb-3050-4070-9c3d-3fb421c60dd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': \"df['Country_name'].tolist()\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['Korea', 'North_Korea', 'China', 'Taiwan', 'Mongolia', 'Hong_Kong', 'Viet_Nam', 'Thailand', 'Singapore', 'Malaysia', 'Brunei', 'Philippines', 'Indonesia', 'Cambodia', 'Lao', 'Myanmar', 'India', 'Pakistan', 'Sri_Lanka', 'Maldives', 'Bangladesh', 'Timor-Leste', 'Macao', 'Afghanistan', 'Nepal', 'Bhutan', 'Iran', 'Iraq', 'Bahrain', 'Saudi_Arabia', 'Kuwait', 'Qatar', 'Oman', 'Israel', 'Jordan', 'Syria', 'Lebanon', 'United_Arab_Emirates', 'Yemen', 'Azerbaijan', 'Armenia', 'Uzbekistan', 'Kazakhstan', 'Kyrgyz', 'Tajikistan', 'Turkmenistan', 'Georgia', 'West_Bank_and_Gaza_Strip', 'Iceland', 'Norway', 'Sweden', 'Denmark', 'United_Kingdom', 'Ireland', 'Netherlands', 'Belgium', 'Luxembourg', 'France', 'Monaco', 'Andorra', 'Germany', 'Switzerland', 'Azores_(Portugal)', 'Portugal', 'Spain', 'Gibraltar_(UK)', 'Italy', 'Malta', 'Finland', 'Poland', 'Russia', 'Austria', 'Hungary', 'Serbia', 'Albania', 'Greece', 'Romania', 'Bulgaria', 'Cyprus', 'Turkey', 'Estonia', 'Latvia', 'Lithuania', 'Ukraine', 'Belarus', 'Moldova', 'Croatia', 'Slovenia', 'Bosnia_and_Herzegovina', 'Former_Yugoslav_Republic_of_Macedonia', 'Czech_Republic', 'Slovak', 'Montenegro', 'Kosovo', 'Greenland_(Denmark)', 'Canada', 'St.Pierre_and_Miquelon_(France)', 'United_States_of_America', 'Mexico', 'Guatemala', 'Honduras', 'Belize', 'El_Salvador', 'Nicaragua', 'Costa_Rica', 'Panama', 'Bermuda_(UK)', 'Bahamas', 'Jamaica', 'Turks_and_Caicos_Islands_(UK)', 'Barbados', 'Trinidad_and_Tobago', 'Cuba', 'Haiti', 'Dominican_Republic', 'Puerto_Rico_(USA)', 'US_Virgin_Islands', 'Netherlands_Antilles', 'French_West_Indies', 'Cayman_islands_(UK)', 'Grenada', 'St.Lucia', 'Antigua_and_Barbuda', 'British_Virgin_Islands', 'Dominica', 'Monstserrat_(UK)', 'St.Christopher_and_Nevis', 'St.Vincent', 'British_Anguilla', 'Colombia', 'Venezuela', 'Guyana', 'Suriname', 'French_Guiana', 'Ecuador', 'Peru', 'Bolivia', 'Chile', 'Brazil', 'Paraguay', 'Uruguay', 'Argentina', 'Falkland_Islands_and_Dependencies_(UK)', 'British_Antarctic_Territory', 'Morocco', 'Ceuta_and_Melilla_(Spain)', 'Algeria', 'Tunisia', 'Libya', 'Egypt', 'Sudan', 'West_Sahara', 'Mauritania', 'Senegal', 'Gambia', 'Guinea-Bissau', 'Guinea', 'Sierra_Leone', 'Liberia', \"Côte_d'Ivoire\", 'Ghana', 'Togo', 'Benin', 'Mali', 'Burkina_Faso', 'Cape_Verde', 'Canary_Islands_(Spain)', 'Nigeria', 'Niger', 'Rwanda', 'Cameroon', 'Chad', 'Central_Africa', 'Equatorial_Guinea', 'Gabon', 'Congo', 'Congo', 'Burundi', 'Angola', 'Sao_Tome_and_Principe', 'St._Helena_Island_and_Dependencies_(UK)', 'Ethiopia', 'Djibouti', 'Somalia', 'Kenya', 'Uganda', 'Tanzania', 'Seychelles', 'Mozambique', 'Madagascar', 'Mauritius', 'Reunion_(France)', 'Zimbabwe', 'Namibia', 'South_Africa', 'Lesotho', 'Malawi', 'Zambia', 'Botswana', 'Swaziland', 'British_indian_Ocean_Territories', 'Comoros', 'Eritrea', 'South_Sudan', 'Australia', 'Papua_New_Guinea', 'Other_Australian_Territories', 'New_Zealand', 'Cook', 'Tokelau_Islands_(NZ)', 'Niue', 'Samoa', 'Vanuatu', 'Fiji', 'Solomon_Islands', 'Tonga', 'Kiribati', 'Pitcairn_(UK)', 'Nauru', 'New_Caledonia_(France)', 'French_Polynesia', 'Guam_(USA)', 'American_Samoa', 'American_Oceania', 'Tuvalu', 'Marshall', 'Micronesia', 'Northern_Mariana_Islands_(USA)', 'Palau', 'For_Order', 'Unknown', 'Bonded_Manufacturing_Warehouse']\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "['Korea', 'North_Korea', 'China', 'Taiwan', 'Mongolia', 'Hong_Kong', 'Viet_Nam', 'Thailand', 'Singapore', 'Malaysia', 'Brunei', 'Philippines', 'Indonesia', 'Cambodia', 'Lao', 'Myanmar', 'India', 'Pakistan', 'Sri_Lanka', 'Maldives', 'Bangladesh', 'Timor-Leste', 'Macao', 'Afghanistan', 'Nepal', 'Bhutan', 'Iran', 'Iraq', 'Bahrain', 'Saudi_Arabia', 'Kuwait', 'Qatar', 'Oman', 'Israel', 'Jordan', 'Syria', 'Lebanon', 'United_Arab_Emirates', 'Yemen', 'Azerbaijan', 'Armenia', 'Uzbekistan', 'Kazakhstan', 'Kyrgyz', 'Tajikistan', 'Turkmenistan', 'Georgia', 'West_Bank_and_Gaza_Strip', 'Iceland', 'Norway', 'Sweden', 'Denmark', 'United_Kingdom', 'Ireland', 'Netherlands', 'Belgium', 'Luxembourg', 'France', 'Monaco', 'Andorra', 'Germany', 'Switzerland', 'Azores_(Portugal)', 'Portugal', 'Spain', 'Gibraltar_(UK)', 'Italy', 'Malta', 'Finland', 'Poland', 'Russia', 'Austria', 'Hungary', 'Serbia', 'Albania', 'Greece', 'Romania', 'Bulgaria', 'Cyprus', 'Turkey', 'Estonia', 'Latvia', 'Lithuania', 'Ukraine', 'Belarus', 'Moldova', 'Croatia', 'Slovenia', 'Bosnia_and_Herzegovina', 'Former_Yugoslav_Republic_of_Macedonia', 'Czech_Republic', 'Slovak', 'Montenegro', 'Kosovo', 'Greenland_(Denmark)', 'Canada', 'St.Pierre_and_Miquelon_(France)', 'United_States_of_America', 'Mexico', 'Guatemala', 'Honduras', 'Belize', 'El_Salvador', 'Nicaragua', 'Costa_Rica', 'Panama', 'Bermuda_(UK)', 'Bahamas', 'Jamaica', 'Turks_and_Caicos_Islands_(UK)', 'Barbados', 'Trinidad_and_Tobago', 'Cuba', 'Haiti', 'Dominican_Republic', 'Puerto_Rico_(USA)', 'US_Virgin_Islands', 'Netherlands_Antilles', 'French_West_Indies', 'Cayman_islands_(UK)', 'Grenada', 'St.Lucia', 'Antigua_and_Barbuda', 'British_Virgin_Islands', 'Dominica', 'Monstserrat_(UK)', 'St.Christopher_and_Nevis', 'St.Vincent', 'British_Anguilla', 'Colombia', 'Venezuela', 'Guyana', 'Suriname', 'French_Guiana', 'Ecuador', 'Peru', 'Bolivia', 'Chile', 'Brazil', 'Paraguay', 'Uruguay', 'Argentina', 'Falkland_Islands_and_Dependencies_(UK)', 'British_Antarctic_Territory', 'Morocco', 'Ceuta_and_Melilla_(Spain)', 'Algeria', 'Tunisia', 'Libya', 'Egypt', 'Sudan', 'West_Sahara', 'Mauritania', 'Senegal', 'Gambia', 'Guinea-Bissau', 'Guinea', 'Sierra_Leone', 'Liberia', \"Côte_d'Ivoire\", 'Ghana', 'Togo', 'Benin', 'Mali', 'Burkina_Faso', 'Cape_Verde', 'Canary_Islands_(Spain)', 'Nigeria', 'Niger', 'Rwanda', 'Cameroon', 'Chad', 'Central_Africa', 'Equatorial_Guinea', 'Gabon', 'Congo', 'Congo', 'Burundi', 'Angola', 'Sao_Tome_and_Principe', 'St._Helena_Island_and_Dependencies_(UK)', 'Ethiopia', 'Djibouti', 'Somalia', 'Kenya', 'Uganda', 'Tanzania', 'Seychelles', 'Mozambique', 'Madagascar', 'Mauritius', 'Reunion_(France)', 'Zimbabwe', 'Namibia', 'South_Africa', 'Lesotho', 'Malawi', 'Zambia', 'Botswana', 'Swaziland', 'British_indian_Ocean_Territories', 'Comoros', 'Eritrea', 'South_Sudan', 'Australia', 'Papua_New_Guinea', 'Other_Australian_Territories', 'New_Zealand', 'Cook', 'Tokelau_Islands_(NZ)', 'Niue', 'Samoa', 'Vanuatu', 'Fiji', 'Solomon_Islands', 'Tonga', 'Kiribati', 'Pitcairn_(UK)', 'Nauru', 'New_Caledonia_(France)', 'French_Polynesia', 'Guam_(USA)', 'American_Samoa', 'American_Oceania', 'Tuvalu', 'Marshall', 'Micronesia', 'Northern_Mariana_Islands_(USA)', 'Palau', 'For_Order', 'Unknown', 'Bonded_Manufacturing_Warehouse']\n"
     ]
    }
   ],
   "source": [
    "res = agent.run(input=\"list all the countries\",)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3dbe6d57-80d5-4f32-b984-269fb4eb7f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': \"df[['Country_name', 'Area']].head(10)\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m  Country_name  Area\n",
      "0        Korea  Asia\n",
      "1  North_Korea  Asia\n",
      "2        China  Asia\n",
      "3       Taiwan  Asia\n",
      "4     Mongolia  Asia\n",
      "5    Hong_Kong  Asia\n",
      "6     Viet_Nam  Asia\n",
      "7     Thailand  Asia\n",
      "8    Singapore  Asia\n",
      "9     Malaysia  Asia\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "  Country_name  Area\n",
      "0        Korea  Asia\n",
      "1  North_Korea  Asia\n",
      "2        China  Asia\n",
      "3       Taiwan  Asia\n",
      "4     Mongolia  Asia\n",
      "5    Hong_Kong  Asia\n",
      "6     Viet_Nam  Asia\n",
      "7     Thailand  Asia\n",
      "8    Singapore  Asia\n",
      "9     Malaysia  Asia\n"
     ]
    }
   ],
   "source": [
    "res = agent.run(input=\" give me the 10 country name and the area both\",)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b43cf1c8-b63c-44e5-8e4e-a691e84ebd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': \"df['Country_name'].head(10).tolist()\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['Korea', 'North_Korea', 'China', 'Taiwan', 'Mongolia', 'Hong_Kong', 'Viet_Nam', 'Thailand', 'Singapore', 'Malaysia']\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `python_repl_ast` with `{'query': \"df['Area'].head(10).tolist()\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['Asia', 'Asia', 'Asia', 'Asia', 'Asia', 'Asia', 'Asia', 'Asia', 'Asia', 'Asia']\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe first 10 country names are:\n",
      "1. Korea\n",
      "2. North_Korea\n",
      "3. China\n",
      "4. Taiwan\n",
      "5. Mongolia\n",
      "6. Hong_Kong\n",
      "7. Viet_Nam\n",
      "8. Thailand\n",
      "9. Singapore\n",
      "10. Malaysia\n",
      "\n",
      "The first 10 areas are:\n",
      "1. Asia\n",
      "2. Asia\n",
      "3. Asia\n",
      "4. Asia\n",
      "5. Asia\n",
      "6. Asia\n",
      "7. Asia\n",
      "8. Asia\n",
      "9. Asia\n",
      "10. Asia\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The first 10 country names are:\n",
      "1. Korea\n",
      "2. North_Korea\n",
      "3. China\n",
      "4. Taiwan\n",
      "5. Mongolia\n",
      "6. Hong_Kong\n",
      "7. Viet_Nam\n",
      "8. Thailand\n",
      "9. Singapore\n",
      "10. Malaysia\n",
      "\n",
      "The first 10 areas are:\n",
      "1. Asia\n",
      "2. Asia\n",
      "3. Asia\n",
      "4. Asia\n",
      "5. Asia\n",
      "6. Asia\n",
      "7. Asia\n",
      "8. Asia\n",
      "9. Asia\n",
      "10. Asia\n"
     ]
    }
   ],
   "source": [
    "res = agent.run(input=\" give me the 10 country name first and then give me the 10 area\",)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93be9c79-62c9-4291-a4cc-4e622b014c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am called\n"
     ]
    }
   ],
   "source": [
    "class MyTest:\n",
    "    def __call__(self):\n",
    "        self._call()\n",
    "\n",
    "    def _call(self):\n",
    "        print('I am called')\n",
    "\n",
    "# Create an instance\n",
    "mt = MyTest()\n",
    "\n",
    "# Call the instance\n",
    "mt()  # This will print 'I am called'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4475667-1c60-42db-8106-df24ca1a1f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': None,\n",
       " 'memory': None,\n",
       " 'verbose': True,\n",
       " 'tags': None,\n",
       " 'metadata': None,\n",
       " 'agent': {'runnable': {'name': None,\n",
       "   'first': {'name': None,\n",
       "    'mapper': {'name': None,\n",
       "     'steps__': {'agent_scratchpad': RunnableLambda(lambda x: message_formatter(x['intermediate_steps']))}}},\n",
       "   'middle': [{'name': None,\n",
       "     'input_variables': ['agent_scratchpad', 'input'],\n",
       "     'optional_variables': [],\n",
       "     'output_parser': None,\n",
       "     'partial_variables': {},\n",
       "     'metadata': None,\n",
       "     'tags': None,\n",
       "     'messages': [{'content': '\\nYou are working with a pandas dataframe in Python. The name of the dataframe is `df`.\\nThis is the result of `print(df.head())`:\\n|    |   Unnamed: 0 |   Country | Country_name   | Area   |\\n|---:|-------------:|----------:|:---------------|:-------|\\n|  0 |            0 |       103 | Korea          | Asia   |\\n|  1 |            1 |       104 | North_Korea    | Asia   |\\n|  2 |            2 |       105 | China          | Asia   |\\n|  3 |            3 |       106 | Taiwan         | Asia   |\\n|  4 |            4 |       107 | Mongolia       | Asia   |',\n",
       "       'additional_kwargs': {},\n",
       "       'response_metadata': {},\n",
       "       'type': 'system',\n",
       "       'name': None,\n",
       "       'id': None},\n",
       "      {'prompt': {'name': None,\n",
       "        'input_variables': ['input'],\n",
       "        'optional_variables': [],\n",
       "        'output_parser': None,\n",
       "        'partial_variables': {},\n",
       "        'metadata': None,\n",
       "        'tags': None,\n",
       "        'template': '{input}',\n",
       "        'template_format': 'f-string',\n",
       "        'validate_template': False,\n",
       "        '_type': 'prompt'},\n",
       "       'additional_kwargs': {}},\n",
       "      {'variable_name': 'agent_scratchpad',\n",
       "       'optional': False,\n",
       "       'n_messages': None}],\n",
       "     'validate_template': False,\n",
       "     '_type': 'chat'},\n",
       "    {'name': None,\n",
       "     'bound': {'model_name': 'gpt-4o-mini-2024-07-18',\n",
       "      'model': 'gpt-4o-mini-2024-07-18',\n",
       "      'stream': False,\n",
       "      'n': 1,\n",
       "      'temperature': 0.7,\n",
       "      'logprobs': False,\n",
       "      '_type': 'openai-chat'},\n",
       "     'kwargs': {'tools': [{'type': 'function',\n",
       "        'function': {'name': 'python_repl_ast',\n",
       "         'description': 'A Python shell. Use this to execute python commands. Input should be a valid python command. When using this tool, sometimes output is abbreviated - make sure it does not look abbreviated before using it in your answer.',\n",
       "         'parameters': {'type': 'object',\n",
       "          'properties': {'query': {'description': 'code snippet to run',\n",
       "            'type': 'string'}},\n",
       "          'required': ['query']}}}]},\n",
       "     'config': {},\n",
       "     'config_factories': [],\n",
       "     'custom_input_type': None,\n",
       "     'custom_output_type': None}],\n",
       "   'last': {'name': None, '_type': 'tools-agent-output-parser'}},\n",
       "  'input_keys_arg': ['input'],\n",
       "  'return_keys_arg': ['output'],\n",
       "  'stream_runnable': True},\n",
       " 'tools': [{'name': 'python_repl_ast',\n",
       "   'description': 'A Python shell. Use this to execute python commands. Input should be a valid python command. When using this tool, sometimes output is abbreviated - make sure it does not look abbreviated before using it in your answer.',\n",
       "   'args_schema': langchain_experimental.tools.python.tool.PythonInputs,\n",
       "   'return_direct': True,\n",
       "   'verbose': False,\n",
       "   'tags': None,\n",
       "   'metadata': None,\n",
       "   'handle_tool_error': False,\n",
       "   'handle_validation_error': False,\n",
       "   'response_format': 'content',\n",
       "   'globals': {'__builtins__': {'__name__': 'builtins',\n",
       "     '__doc__': \"Built-in functions, exceptions, and other objects.\\n\\nNoteworthy: None is the `nil' object; Ellipsis represents `...' in slices.\",\n",
       "     '__package__': '',\n",
       "     '__loader__': _frozen_importlib.BuiltinImporter,\n",
       "     '__spec__': ModuleSpec(name='builtins', loader=<class '_frozen_importlib.BuiltinImporter'>, origin='built-in'),\n",
       "     '__build_class__': <function __build_class__>,\n",
       "     '__import__': <function __import__>,\n",
       "     'abs': <function abs(x, /)>,\n",
       "     'all': <function all(iterable, /)>,\n",
       "     'any': <function any(iterable, /)>,\n",
       "     'ascii': <function ascii(obj, /)>,\n",
       "     'bin': <function bin(number, /)>,\n",
       "     'breakpoint': <function breakpoint>,\n",
       "     'callable': <function callable(obj, /)>,\n",
       "     'chr': <function chr(i, /)>,\n",
       "     'compile': <function compile(source, filename, mode, flags=0, dont_inherit=False, optimize=-1, *, _feature_version=-1)>,\n",
       "     'delattr': <function delattr(obj, name, /)>,\n",
       "     'dir': <function dir>,\n",
       "     'divmod': <function divmod(x, y, /)>,\n",
       "     'eval': <function eval(source, globals=None, locals=None, /)>,\n",
       "     'exec': <function exec(source, globals=None, locals=None, /)>,\n",
       "     'format': <function format(value, format_spec='', /)>,\n",
       "     'getattr': <function getattr>,\n",
       "     'globals': <function globals()>,\n",
       "     'hasattr': <function hasattr(obj, name, /)>,\n",
       "     'hash': <function hash(obj, /)>,\n",
       "     'hex': <function hex(number, /)>,\n",
       "     'id': <function id(obj, /)>,\n",
       "     'input': <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7f2031582fa0>>,\n",
       "     'isinstance': <function isinstance(obj, class_or_tuple, /)>,\n",
       "     'issubclass': <function issubclass(cls, class_or_tuple, /)>,\n",
       "     'iter': <function iter>,\n",
       "     'len': <function len(obj, /)>,\n",
       "     'locals': <function locals()>,\n",
       "     'max': <function max>,\n",
       "     'min': <function min>,\n",
       "     'next': <function next>,\n",
       "     'oct': <function oct(number, /)>,\n",
       "     'ord': <function ord(c, /)>,\n",
       "     'pow': <function pow(base, exp, mod=None)>,\n",
       "     'print': <function print>,\n",
       "     'repr': <function repr(obj, /)>,\n",
       "     'round': <function round(number, ndigits=None)>,\n",
       "     'setattr': <function setattr(obj, name, value, /)>,\n",
       "     'sorted': <function sorted(iterable, /, *, key=None, reverse=False)>,\n",
       "     'sum': <function sum(iterable, /, start=0)>,\n",
       "     'vars': <function vars>,\n",
       "     'None': None,\n",
       "     'Ellipsis': Ellipsis,\n",
       "     'NotImplemented': NotImplemented,\n",
       "     'False': False,\n",
       "     'True': True,\n",
       "     'bool': bool,\n",
       "     'memoryview': memoryview,\n",
       "     'bytearray': bytearray,\n",
       "     'bytes': bytes,\n",
       "     'classmethod': classmethod,\n",
       "     'complex': complex,\n",
       "     'dict': dict,\n",
       "     'enumerate': enumerate,\n",
       "     'filter': filter,\n",
       "     'float': float,\n",
       "     'frozenset': frozenset,\n",
       "     'property': property,\n",
       "     'int': int,\n",
       "     'list': list,\n",
       "     'map': map,\n",
       "     'object': object,\n",
       "     'range': range,\n",
       "     'reversed': reversed,\n",
       "     'set': set,\n",
       "     'slice': slice,\n",
       "     'staticmethod': staticmethod,\n",
       "     'str': str,\n",
       "     'super': super,\n",
       "     'tuple': tuple,\n",
       "     'type': type,\n",
       "     'zip': zip,\n",
       "     '__debug__': True,\n",
       "     'BaseException': BaseException,\n",
       "     'Exception': Exception,\n",
       "     'TypeError': TypeError,\n",
       "     'StopAsyncIteration': StopAsyncIteration,\n",
       "     'StopIteration': StopIteration,\n",
       "     'GeneratorExit': GeneratorExit,\n",
       "     'SystemExit': SystemExit,\n",
       "     'KeyboardInterrupt': KeyboardInterrupt,\n",
       "     'ImportError': ImportError,\n",
       "     'ModuleNotFoundError': ModuleNotFoundError,\n",
       "     'OSError': OSError,\n",
       "     'EnvironmentError': OSError,\n",
       "     'IOError': OSError,\n",
       "     'EOFError': EOFError,\n",
       "     'RuntimeError': RuntimeError,\n",
       "     'RecursionError': RecursionError,\n",
       "     'NotImplementedError': NotImplementedError,\n",
       "     'NameError': NameError,\n",
       "     'UnboundLocalError': UnboundLocalError,\n",
       "     'AttributeError': AttributeError,\n",
       "     'SyntaxError': SyntaxError,\n",
       "     'IndentationError': IndentationError,\n",
       "     'TabError': TabError,\n",
       "     'LookupError': LookupError,\n",
       "     'IndexError': IndexError,\n",
       "     'KeyError': KeyError,\n",
       "     'ValueError': ValueError,\n",
       "     'UnicodeError': UnicodeError,\n",
       "     'UnicodeEncodeError': UnicodeEncodeError,\n",
       "     'UnicodeDecodeError': UnicodeDecodeError,\n",
       "     'UnicodeTranslateError': UnicodeTranslateError,\n",
       "     'AssertionError': AssertionError,\n",
       "     'ArithmeticError': ArithmeticError,\n",
       "     'FloatingPointError': FloatingPointError,\n",
       "     'OverflowError': OverflowError,\n",
       "     'ZeroDivisionError': ZeroDivisionError,\n",
       "     'SystemError': SystemError,\n",
       "     'ReferenceError': ReferenceError,\n",
       "     'MemoryError': MemoryError,\n",
       "     'BufferError': BufferError,\n",
       "     'Warning': Warning,\n",
       "     'UserWarning': UserWarning,\n",
       "     'DeprecationWarning': DeprecationWarning,\n",
       "     'PendingDeprecationWarning': PendingDeprecationWarning,\n",
       "     'SyntaxWarning': SyntaxWarning,\n",
       "     'RuntimeWarning': RuntimeWarning,\n",
       "     'FutureWarning': FutureWarning,\n",
       "     'ImportWarning': ImportWarning,\n",
       "     'UnicodeWarning': UnicodeWarning,\n",
       "     'BytesWarning': BytesWarning,\n",
       "     'ResourceWarning': ResourceWarning,\n",
       "     'ConnectionError': ConnectionError,\n",
       "     'BlockingIOError': BlockingIOError,\n",
       "     'BrokenPipeError': BrokenPipeError,\n",
       "     'ChildProcessError': ChildProcessError,\n",
       "     'ConnectionAbortedError': ConnectionAbortedError,\n",
       "     'ConnectionRefusedError': ConnectionRefusedError,\n",
       "     'ConnectionResetError': ConnectionResetError,\n",
       "     'FileExistsError': FileExistsError,\n",
       "     'FileNotFoundError': FileNotFoundError,\n",
       "     'IsADirectoryError': IsADirectoryError,\n",
       "     'NotADirectoryError': NotADirectoryError,\n",
       "     'InterruptedError': InterruptedError,\n",
       "     'PermissionError': PermissionError,\n",
       "     'ProcessLookupError': ProcessLookupError,\n",
       "     'TimeoutError': TimeoutError,\n",
       "     'open': <function io.open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)>,\n",
       "     'copyright': Copyright (c) 2001-2021 Python Software Foundation.\n",
       "     All Rights Reserved.\n",
       "     \n",
       "     Copyright (c) 2000 BeOpen.com.\n",
       "     All Rights Reserved.\n",
       "     \n",
       "     Copyright (c) 1995-2001 Corporation for National Research Initiatives.\n",
       "     All Rights Reserved.\n",
       "     \n",
       "     Copyright (c) 1991-1995 Stichting Mathematisch Centrum, Amsterdam.\n",
       "     All Rights Reserved.,\n",
       "     'credits':     Thanks to CWI, CNRI, BeOpen.com, Zope Corporation and a cast of thousands\n",
       "         for supporting Python development.  See www.python.org for more information.,\n",
       "     'license': Type license() to see the full license text,\n",
       "     'help': Type help() for interactive help, or help(object) for help about object.,\n",
       "     'execfile': <function _pydev_bundle._pydev_execfile.execfile(file, glob=None, loc=None)>,\n",
       "     'runfile': <function _pydev_bundle.pydev_umd.runfile(filename, args=None, wdir=None, namespace=None)>,\n",
       "     '__IPYTHON__': True,\n",
       "     'display': <function IPython.core.display_functions.display(*objs, include=None, exclude=None, metadata=None, transient=None, display_id=None, raw=False, clear=False, **kwargs)>,\n",
       "     'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f2030086e20>>}},\n",
       "   'locals': {'df':      Unnamed: 0  Country                    Country_name  \\\n",
       "    0             0      103                           Korea   \n",
       "    1             1      104                     North_Korea   \n",
       "    2             2      105                           China   \n",
       "    3             3      106                          Taiwan   \n",
       "    4             4      107                        Mongolia   \n",
       "    ..          ...      ...                             ...   \n",
       "    227         227      627  Northern_Mariana_Islands_(USA)   \n",
       "    228         228      628                           Palau   \n",
       "    229         229      701                       For_Order   \n",
       "    230         230      702                         Unknown   \n",
       "    231         231      703  Bonded_Manufacturing_Warehouse   \n",
       "    \n",
       "                                     Area  \n",
       "    0                                Asia  \n",
       "    1                                Asia  \n",
       "    2                                Asia  \n",
       "    3                                Asia  \n",
       "    4                                Asia  \n",
       "    ..                                ...  \n",
       "    227                           Oceania  \n",
       "    228                           Oceania  \n",
       "    229                      Special_Area  \n",
       "    230                      Special_Area  \n",
       "    231  Integrated_Hozei_Ar_Special_Area  \n",
       "    \n",
       "    [232 rows x 4 columns]},\n",
       "   'sanitize_input': True}],\n",
       " 'return_intermediate_steps': False,\n",
       " 'max_iterations': 15,\n",
       " 'max_execution_time': None,\n",
       " 'early_stopping_method': 'force',\n",
       " 'handle_parsing_errors': False,\n",
       " 'trim_intermediate_steps': -1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f9b16b-5140-46d4-bff1-403f785f3908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eaeda7c4-f216-4caf-9a12-3fde583cb064",
   "metadata": {},
   "source": [
    "- _iter_next_step :  Override this to take control of how the agent makes and acts on choices.Take a single step in the thought-action-observation loop\n",
    "    - inputs: Dict[str, str] \n",
    "    - intermediate_steps: List[Tuple[AgentAction, str]]\n",
    "    - intermediate_steps = self._prepare_intermediate_steps(intermediate_steps)\n",
    "    - output = self.agent.plan(intermediate_steps,callbacks=run_manager.get_child() if run_manager else None,\n",
    "                **inputs, )\n",
    "    \n",
    "- self._perform_agent_action-> AgentStep:\n",
    "    - tool_run_kwargs = self.agent.tool_run_logging_kwargs()\n",
    "    - if return_direct: tool_run_kwargs[\"llm_prefix\"] = \"\"\n",
    "    - observation = tool.run(agent_action.tool_input, **tool_run_kwargs)\n",
    "    - AgentStep(action=agent_action, observation=observation)\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8f9c24-7414-444e-a731-7e09faa54393",
   "metadata": {},
   "source": [
    "https://github.com/langchain-ai/langchain/blob/02c35da445b5a8cbd3d47ba468243f9297795124/libs/langchain/langchain/agents/react/agent.py#L13\n",
    "def create_react_agent(\n",
    "    llm: BaseLanguageModel,\n",
    "    tools: Sequence[BaseTool],\n",
    "    prompt: BasePromptTemplate,\n",
    "    output_parser: Optional[AgentOutputParser] = None,\n",
    "    tools_renderer: ToolsRenderer = render_text_description,\n",
    "    *,\n",
    "    stop_sequence: Union[bool, List[str]] = True,\n",
    ") -> Runnable:\n",
    "    \"\"\"Create an agent that uses ReAct prompting.\n",
    "\n",
    "    Based on paper \"ReAct: Synergizing Reasoning and Acting in Language Models\"\n",
    "    (https://arxiv.org/abs/2210.03629)\n",
    "\n",
    "    Args:\n",
    "        llm: LLM to use as the agent.\n",
    "        tools: Tools this agent has access to.\n",
    "        prompt: The prompt to use. See Prompt section below for more.\n",
    "        output_parser: AgentOutputParser for parse the LLM output.\n",
    "        tools_renderer: This controls how the tools are converted into a string and\n",
    "            then passed into the LLM. Default is `render_text_description`.\n",
    "        stop_sequence: bool or list of str.\n",
    "            If True, adds a stop token of \"Observation:\" to avoid hallucinates.\n",
    "            If False, does not add a stop token.\n",
    "            If a list of str, uses the provided list as the stop tokens.\n",
    "\n",
    "            Default is True. You may to set this to False if the LLM you are using\n",
    "            does not support stop sequences.\n",
    "\n",
    "    Returns:\n",
    "        A Runnable sequence representing an agent. It takes as input all the same input\n",
    "        variables as the prompt passed in does. It returns as output either an\n",
    "        AgentAction or AgentFinish.\n",
    "\n",
    "    Examples:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            from langchain import hub\n",
    "            from langchain_community.llms import OpenAI\n",
    "            from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "            prompt = hub.pull(\"hwchase17/react\")\n",
    "            model = OpenAI()\n",
    "            tools = ...\n",
    "\n",
    "            agent = create_react_agent(model, tools, prompt)\n",
    "            agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "            agent_executor.invoke({\"input\": \"hi\"})\n",
    "\n",
    "            # Use with chat history\n",
    "            from langchain_core.messages import AIMessage, HumanMessage\n",
    "            agent_executor.invoke(\n",
    "                {\n",
    "                    \"input\": \"what's my name?\",\n",
    "                    # Notice that chat_history is a string\n",
    "                    # since this prompt is aimed at LLMs, not chat models\n",
    "                    \"chat_history\": \"Human: My name is Bob\\\\nAI: Hello Bob!\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    Prompt:\n",
    "\n",
    "        The prompt must have input keys:\n",
    "            * `tools`: contains descriptions and arguments for each tool.\n",
    "            * `tool_names`: contains all tool names.\n",
    "            * `agent_scratchpad`: contains previous agent actions and tool outputs as a string.\n",
    "\n",
    "        Here's an example:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "            template = '''Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "            {tools}\n",
    "\n",
    "            Use the following format:\n",
    "\n",
    "            Question: the input question you must answer\n",
    "            Thought: you should always think about what to do\n",
    "            Action: the action to take, should be one of [{tool_names}]\n",
    "            Action Input: the input to the action\n",
    "            Observation: the result of the action\n",
    "            ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "            Thought: I now know the final answer\n",
    "            Final Answer: the final answer to the original input question\n",
    "\n",
    "            Begin!\n",
    "\n",
    "            Question: {input}\n",
    "            Thought:{agent_scratchpad}'''\n",
    "\n",
    "            prompt = PromptTemplate.from_template(template)\n",
    "    \"\"\"  # noqa: E501\n",
    "    missing_vars = {\"tools\", \"tool_names\", \"agent_scratchpad\"}.difference(\n",
    "        prompt.input_variables + list(prompt.partial_variables)\n",
    "    )\n",
    "    if missing_vars:\n",
    "        raise ValueError(f\"Prompt missing required variables: {missing_vars}\")\n",
    "\n",
    "    prompt = prompt.partial(\n",
    "        tools=tools_renderer(list(tools)),\n",
    "        tool_names=\", \".join([t.name for t in tools]),\n",
    "    )\n",
    "    if stop_sequence:\n",
    "        stop = [\"\\nObservation\"] if stop_sequence is True else stop_sequence\n",
    "        llm_with_stop = llm.bind(stop=stop)  ##### UNDERSTAND THIS IN DETAIL ############################\n",
    "    else:\n",
    "        llm_with_stop = llm\n",
    "    output_parser = output_parser or ReActSingleInputOutputParser()\n",
    "    agent = (\n",
    "        RunnablePassthrough.assign(\n",
    "            agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "        )\n",
    "        | prompt\n",
    "        | llm_with_stop\n",
    "        | output_parser\n",
    "    )\n",
    "    return agent\n",
    "    \n",
    "SINCE LLM IS A RUNNABEL WE UNDERSTAND RUNNNABLE AND THE BIND OPERATION TO RUNNABLE FIRST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e55bffae-53fb-46c4-960d-a86e5a0d90e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools_renderer: ToolsRenderer = render_text_description\n",
    "# https://github.com/langchain-ai/langchain/blob/02c35da445b5a8cbd3d47ba468243f9297795124/libs/core/langchain_core/tools.py#L1352\n",
    "from inspect import signature\n",
    "from typing import List\n",
    "def render_text_description_and_args(tools) -> str:\n",
    "    \"\"\"Render the tool name, description, and args in plain text.\n",
    "\n",
    "    Args:\n",
    "        tools: The tools to render.\n",
    "\n",
    "    Returns:\n",
    "        The rendered text.\n",
    "\n",
    "    Output will be in the format of:\n",
    "\n",
    "    .. code-block:: markdown\n",
    "\n",
    "        search: This tool is used for search, args: {\"query\": {\"type\": \"string\"}}\n",
    "        calculator: This tool is used for math, \\\n",
    "args: {\"expression\": {\"type\": \"string\"}}\n",
    "    \"\"\"\n",
    "    tool_strings = []\n",
    "    for tool in tools:\n",
    "        args_schema = str(tool.args)\n",
    "        if hasattr(tool, \"func\") and tool.func:\n",
    "            sig = signature(tool.func)\n",
    "            description = f\"{tool.name}{sig} - {tool.description}\"\n",
    "        else:\n",
    "            description = f\"{tool.name} - {tool.description}\"\n",
    "        tool_strings.append(f\"{description}, args: {args_schema}\")\n",
    "    return \"\\n\".join(tool_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9989501-432d-49cd-829b-f280f3c30bcd",
   "metadata": {},
   "source": [
    "https://github.com/langchain-ai/langchain/blob/02c35da445b5a8cbd3d47ba468243f9297795124/libs/core/langchain_core/tools.py#L1352"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3f1f246d-3d01-4f9a-8163-592f36718fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import BaseTool\n",
    "def render_text_description(tools: List[BaseTool]) -> str:\n",
    "    \"\"\"Render the tool name and description in plain text.\n",
    "\n",
    "    Args:\n",
    "        tools: The tools to render.\n",
    "\n",
    "    Returns:\n",
    "        The rendered text.\n",
    "\n",
    "    Output will be in the format of:\n",
    "\n",
    "    .. code-block:: markdown\n",
    "\n",
    "        search: This tool is used for search\n",
    "        calculator: This tool is used for math\n",
    "    \"\"\"\n",
    "    descriptions = []\n",
    "    for tool in tools:\n",
    "        if hasattr(tool, \"func\") and tool.func:\n",
    "            sig = signature(tool.func)\n",
    "            description = f\"{tool.name}{sig} - {tool.description}\"\n",
    "        else:\n",
    "            description = f\"{tool.name} - {tool.description}\"\n",
    "\n",
    "        descriptions.append(description)\n",
    "    return \"\\n\".join(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c927d95-237a-4a7c-a7d9-c8c1a4d64beb",
   "metadata": {},
   "source": [
    "https://github.com/langchain-ai/langchain/blob/02c35da445b5a8cbd3d47ba468243f9297795124/libs/langchain/langchain/agents/format_scratchpad/log.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8238ba60-b8c4-4ea1-9db9-4d94e758dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "from langchain_core.agents import AgentAction\n",
    "\n",
    "\n",
    "def format_log_to_str(\n",
    "    intermediate_steps: List[Tuple[AgentAction, str]],\n",
    "    observation_prefix: str = \"Observation: \",\n",
    "    llm_prefix: str = \"Thought: \",\n",
    ") -> str:\n",
    "    \"\"\"Construct the scratchpad that lets the agent continue its thought process.\n",
    "\n",
    "    Args:\n",
    "        intermediate_steps: List of tuples of AgentAction and observation strings.\n",
    "        observation_prefix: Prefix to append the observation with.\n",
    "             Defaults to \"Observation: \".\n",
    "        llm_prefix: Prefix to append the llm call with.\n",
    "                Defaults to \"Thought: \".\n",
    "\n",
    "    Returns:\n",
    "        str: The scratchpad.\n",
    "    \"\"\"\n",
    "    thoughts = \"\"\n",
    "    for action, observation in intermediate_steps:\n",
    "        thoughts += action.log\n",
    "        thoughts += f\"\\n{observation_prefix}{observation}\\n{llm_prefix}\"\n",
    "    return thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3713b5ea-85b2-46a0-8997-114b9be9e123",
   "metadata": {},
   "source": [
    "- In this case agent is just a chain of:  input_data_dict | passthrough_with_addl_key | promt| model| parser\n",
    "- The input_data_dict is having a key: intermidiate_steps\n",
    "- passthrough_with_addl_key: agent_scratchpad whose value is formatted thought and observation intermidiate_steps\n",
    "- this dict goes to prompt to fill up the variables in the prmpt\n",
    "- prompt goes to model\n",
    "- output of the model is parsed\n",
    "- we need to find who is calling this agent.invoke with the assumed input_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e1c75c50-0a87-4ff7-ac0f-6590b17625fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.prompts import BasePromptTemplate\n",
    "from langchain.agents import AgentOutputParser\n",
    "from typing import List, Optional, Sequence, Union\n",
    "from langchain.tools.render import ToolsRenderer, render_text_description\n",
    "\n",
    "def create_react_agent(\n",
    "    llm: BaseLanguageModel,\n",
    "    tools: Sequence[BaseTool],\n",
    "    prompt: BasePromptTemplate,\n",
    "    output_parser: Optional[AgentOutputParser] = None,\n",
    "    tools_renderer: ToolsRenderer = render_text_description,\n",
    "    *,\n",
    "    stop_sequence: Union[bool, List[str]] = True,\n",
    ") -> Runnable:\n",
    "    missing_vars = {\"tools\", \"tool_names\", \"agent_scratchpad\"}.difference(\n",
    "            prompt.input_variables + list(prompt.partial_variables)\n",
    "        )\n",
    "    if missing_vars:\n",
    "        raise ValueError(f\"Prompt missing required variables: {missing_vars}\")\n",
    "\n",
    "    prompt = prompt.partial(\n",
    "        tools=tools_renderer(list(tools)),\n",
    "        tool_names=\", \".join([t.name for t in tools]),\n",
    "    )\n",
    "    if stop_sequence:\n",
    "        stop = [\"\\nObservation\"] if stop_sequence is True else stop_sequence\n",
    "        llm_with_stop = llm.bind(stop=stop)\n",
    "    else:\n",
    "        llm_with_stop = llm\n",
    "    output_parser = output_parser or ReActSingleInputOutputParser()\n",
    "    agent = (\n",
    "        RunnablePassthrough.assign(\n",
    "            agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "        )\n",
    "        | prompt\n",
    "        | llm_with_stop\n",
    "        | output_parser\n",
    "    )\n",
    "    return agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "23f5581b-7002-4e04-84bc-bdcfe08ecb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"llmapslab\")\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import load_configs\n",
    "import importlib\n",
    "importlib.reload(load_configs)\n",
    "from load_configs import (\n",
    "    openai_api_key,\n",
    "    llama_api_key, ai21_api_key, \n",
    "    gemini_api_key\n",
    ")\n",
    "from openai_client import call_openai_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c09538e8-9490-4f25-bef0-e4998168aa14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/myDev/llmapps/venv/lib/python3.9/site-packages/langchain/hub.py:86: DeprecationWarning: The `langchainhub sdk` is deprecated.\n",
      "Please use the `langsmith sdk` instead:\n",
      "  pip install langsmith\n",
      "Use the `pull_prompt` method.\n",
      "  res_dict = client.pull_repo(owner_repo_commit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"hi\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad> > chain:RunnableParallel<agent_scratchpad>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"agent_scratchpad\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > chain:RunnableAssign<agent_scratchpad>] [20ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"input\": \"hi\",\n",
      "  \"intermediate_steps\": [],\n",
      "  \"agent_scratchpad\": \"\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"hi\",\n",
      "  \"intermediate_steps\": [],\n",
      "  \"agent_scratchpad\": \"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > llm:OpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Answer the following questions as best you can. You have access to the following tools:\\n\\n\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of []\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: hi\\nThought:\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/myDev/llmapps/venv/lib/python3.9/site-packages/pydantic/main.py:1087: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      "/mnt/d/myDev/llmapps/venv/lib/python3.9/site-packages/pydantic/main.py:1087: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      "/mnt/d/myDev/llmapps/venv/lib/python3.9/site-packages/pydantic/main.py:1087: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      "/mnt/d/myDev/llmapps/venv/lib/python3.9/site-packages/pydantic/main.py:1087: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      "/mnt/d/myDev/llmapps/venv/lib/python3.9/site-packages/pydantic/main.py:1087: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      "/mnt/d/myDev/llmapps/venv/lib/python3.9/site-packages/pydantic/main.py:1087: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      "/mnt/d/myDev/llmapps/venv/lib/python3.9/site-packages/pydantic/main.py:1087: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      "/mnt/d/myDev/llmapps/venv/lib/python3.9/site-packages/pydantic/main.py:1087: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      "/mnt/d/myDev/llmapps/venv/lib/python3.9/site-packages/pydantic/main.py:1087: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      "/mnt/d/myDev/llmapps/venv/lib/python3.9/site-packages/pydantic/main.py:1087: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > llm:OpenAI] [767ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" You should always greet and acknowledge someone when they say hi\\nAction: [print]\\nAction Input: \\\"Hello!\\\"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > parser:ReActSingleInputOutputParser] Entering Parser run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \" You should always greet and acknowledge someone when they say hi\\nAction: [print]\\nAction Input: \\\"Hello!\\\"\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence > parser:ReActSingleInputOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:RunnableSequence] [816ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:invalid_tool] Entering Tool run with input:\n",
      "\u001b[0m\"{'requested_tool_name': '[print]', 'available_tool_names': []}\"\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:invalid_tool] [0ms] Exiting Tool run with output:\n",
      "\u001b[0m\"[print] is not a valid tool, try one of [].\"\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor] [853ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"Agent stopped due to iteration limit or time limit.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/myDev/llmapps/venv/lib/python3.9/site-packages/pydantic/main.py:1087: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      "/mnt/d/myDev/llmapps/venv/lib/python3.9/site-packages/pydantic/main.py:1087: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      "/mnt/d/myDev/llmapps/venv/lib/python3.9/site-packages/pydantic/main.py:1087: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      "/mnt/d/myDev/llmapps/venv/lib/python3.9/site-packages/pydantic/main.py:1087: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      "/mnt/d/myDev/llmapps/venv/lib/python3.9/site-packages/pydantic/main.py:1087: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n",
      "/mnt/d/myDev/llmapps/venv/lib/python3.9/site-packages/pydantic/main.py:1087: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.8/migration/\n",
      "  warnings.warn('The `dict` method is deprecated; use `model_dump` instead.', category=PydanticDeprecatedSince20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'hi',\n",
       " 'output': 'Agent stopped due to iteration limit or time limit.'}"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install langchainhub\n",
    "from langchain import hub\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "model = OpenAI(openai_api_key=openai_api_key)\n",
    "tools = []\n",
    "\n",
    "agent = create_react_agent(model, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, max_iterations=1)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"hi\"})\n",
    "\n",
    "# # Use with chat history\n",
    "# from langchain_core.messages import AIMessage, HumanMessage\n",
    "# agent_executor.invoke(\n",
    "#     {\n",
    "#         \"input\": \"what's my name?\",\n",
    "#         # Notice that chat_history is a string\n",
    "#         # since this prompt is aimed at LLMs, not chat models\n",
    "#         \"chat_history\": \"Human: My name is Bob\\\\nAI: Hello Bob!\",\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "08e746b2-462c-48f4-9503-92cfe737645e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class AgentExecutor in module langchain.agents.agent:\n",
      "\n",
      "class AgentExecutor(langchain.chains.base.Chain)\n",
      " |  AgentExecutor(*, name: Optional[str] = None, memory: Optional[langchain_core.memory.BaseMemory] = None, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, verbose: bool = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, callback_manager: Optional[langchain_core.callbacks.base.BaseCallbackManager] = None, agent: Union[langchain.agents.agent.BaseSingleActionAgent, langchain.agents.agent.BaseMultiActionAgent], tools: Sequence[langchain_core.tools.BaseTool], return_intermediate_steps: bool = False, max_iterations: Optional[int] = 15, max_execution_time: Optional[float] = None, early_stopping_method: str = 'force', handle_parsing_errors: Union[bool, str, Callable[[langchain_core.exceptions.OutputParserException], str]] = False, trim_intermediate_steps: Union[int, Callable[[List[Tuple[langchain_core.agents.AgentAction, str]]], List[Tuple[langchain_core.agents.AgentAction, str]]]] = -1) -> None\n",
      " |  \n",
      " |  Agent that is using tools.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      AgentExecutor\n",
      " |      langchain.chains.base.Chain\n",
      " |      langchain_core.runnables.base.RunnableSerializable\n",
      " |      langchain_core.load.serializable.Serializable\n",
      " |      pydantic.v1.main.BaseModel\n",
      " |      pydantic.v1.utils.Representation\n",
      " |      langchain_core.runnables.base.Runnable\n",
      " |      typing.Generic\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  async astream(self, input: 'Union[Dict[str, Any], Any]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'AsyncIterator[AddableDict]'\n",
      " |      Async enables streaming over steps taken to reach final output.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: Input to the agent.\n",
      " |          config: Config to use.\n",
      " |          **kwargs: Additional arguments.\n",
      " |      \n",
      " |      Yields:\n",
      " |          AddableDict: Addable dictionary.\n",
      " |  \n",
      " |  iter(self, inputs: 'Any', callbacks: 'Callbacks' = None, *, include_run_info: 'bool' = False, async_: 'bool' = False) -> 'AgentExecutorIterator'\n",
      " |      Enables iteration over steps taken to reach final output.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Inputs to the agent.\n",
      " |          callbacks: Callbacks to run.\n",
      " |          include_run_info: Whether to include run info.\n",
      " |          async_: Whether to run async. (Ignored)\n",
      " |      \n",
      " |      Returns:\n",
      " |          AgentExecutorIterator: Agent executor iterator object.\n",
      " |  \n",
      " |  lookup_tool(self, name: 'str') -> 'BaseTool'\n",
      " |      Lookup tool by name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name: Name of tool.\n",
      " |      \n",
      " |      Returns:\n",
      " |          BaseTool: Tool object.\n",
      " |  \n",
      " |  save(self, file_path: 'Union[Path, str]') -> 'None'\n",
      " |      Raise error - saving not supported for Agent Executors.\n",
      " |      \n",
      " |      Args:\n",
      " |          file_path: Path to save to.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: Saving not supported for agent executors.\n",
      " |  \n",
      " |  save_agent(self, file_path: 'Union[Path, str]') -> 'None'\n",
      " |      Save the underlying agent.\n",
      " |      \n",
      " |      Args:\n",
      " |          file_path: Path to save to.\n",
      " |  \n",
      " |  stream(self, input: 'Union[Dict[str, Any], Any]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Iterator[AddableDict]'\n",
      " |      Enables streaming over steps taken to reach final output.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: Input to the agent.\n",
      " |          config: Config to use.\n",
      " |          **kwargs: Additional arguments.\n",
      " |      \n",
      " |      Yields:\n",
      " |          AddableDict: Addable dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_agent_and_tools(agent: 'Union[BaseSingleActionAgent, BaseMultiActionAgent]', tools: 'Sequence[BaseTool]', callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'AgentExecutor' from pydantic.v1.main.ModelMetaclass\n",
      " |      Create from agent and tools.\n",
      " |      \n",
      " |      Args:\n",
      " |          agent: Agent to use.\n",
      " |          tools: Tools to use.\n",
      " |          callbacks: Callbacks to use.\n",
      " |          **kwargs: Additional arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          AgentExecutor: Agent executor object.\n",
      " |  \n",
      " |  validate_runnable_agent(values: 'Dict') -> 'Dict' from pydantic.v1.main.ModelMetaclass\n",
      " |      Convert runnable to agent if passed in.\n",
      " |      \n",
      " |      Args:\n",
      " |          values: Values to validate.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Dict: Validated values.\n",
      " |  \n",
      " |  validate_tools(values: 'Dict') -> 'Dict' from pydantic.v1.main.ModelMetaclass\n",
      " |      Validate that tools are compatible with agent.\n",
      " |      \n",
      " |      Args:\n",
      " |          values: Values to validate.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Dict: Validated values.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If allowed tools are different than provided tools.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __json_encoder__ = pydantic_encoder(obj: Any) -> Any\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  input_keys\n",
      " |      Return the input keys.\n",
      " |      \n",
      " |      :meta private:\n",
      " |  \n",
      " |  output_keys\n",
      " |      Return the singular output key.\n",
      " |      \n",
      " |      :meta private:\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'agent': 'Union[BaseSingleActionAgent, BaseMultiAct...\n",
      " |  \n",
      " |  __class_vars__ = set()\n",
      " |  \n",
      " |  __config__ = <class 'pydantic.v1.config.Config'>\n",
      " |  \n",
      " |  __custom_root_type__ = False\n",
      " |  \n",
      " |  __exclude_fields__ = {'callback_manager': True, 'callbacks': True}\n",
      " |  \n",
      " |  __fields__ = {'agent': ModelField(name='agent', type=Union[BaseSingleA...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  __include_fields__ = None\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  __post_root_validators__ = [(True, <function AgentExecutor.validate_to...\n",
      " |  \n",
      " |  __pre_root_validators__ = [<function Chain.raise_callback_manager_depr...\n",
      " |  \n",
      " |  __private_attributes__ = {}\n",
      " |  \n",
      " |  __schema_cache__ = {}\n",
      " |  \n",
      " |  __signature__ = <Signature (*, name: Optional[str] = None, memor...n_c...\n",
      " |  \n",
      " |  __validators__ = {'verbose': [<pydantic.v1.class_validators.Validator ...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain.chains.base.Chain:\n",
      " |  \n",
      " |  __call__(self, inputs: Union[Dict[str, Any], Any], return_only_outputs: bool = False, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, *, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, run_name: Optional[str] = None, include_run_info: bool = False) -> Dict[str, Any]\n",
      " |      [*Deprecated*] Execute the chain.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Dictionary of inputs, or single input if chain expects\n",
      " |              only one param. Should contain all inputs specified in\n",
      " |              `Chain.input_keys` except for inputs that will be set by the chain's\n",
      " |              memory.\n",
      " |          return_only_outputs: Whether to return only outputs in the\n",
      " |              response. If True, only new keys generated by this chain will be\n",
      " |              returned. If False, both input keys and new keys generated by this\n",
      " |              chain will be returned. Defaults to False.\n",
      " |          callbacks: Callbacks to use for this chain run. These will be called in\n",
      " |              addition to callbacks passed to the chain during construction, but only\n",
      " |              these runtime callbacks will propagate to calls to other objects.\n",
      " |          tags: List of string tags to pass to all callbacks. These will be passed in\n",
      " |              addition to tags passed to the chain during construction, but only\n",
      " |              these runtime tags will propagate to calls to other objects.\n",
      " |          metadata: Optional metadata associated with the chain. Defaults to None\n",
      " |          include_run_info: Whether to include run info in the response. Defaults\n",
      " |              to False.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dict of named outputs. Should contain all outputs specified in\n",
      " |              `Chain.output_keys`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: langchain==0.1.0\n",
      " |         Use invoke instead.\n",
      " |  \n",
      " |  async acall(self, inputs: Union[Dict[str, Any], Any], return_only_outputs: bool = False, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, *, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, run_name: Optional[str] = None, include_run_info: bool = False) -> Dict[str, Any]\n",
      " |      [*Deprecated*] Asynchronously execute the chain.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Dictionary of inputs, or single input if chain expects\n",
      " |              only one param. Should contain all inputs specified in\n",
      " |              `Chain.input_keys` except for inputs that will be set by the chain's\n",
      " |              memory.\n",
      " |          return_only_outputs: Whether to return only outputs in the\n",
      " |              response. If True, only new keys generated by this chain will be\n",
      " |              returned. If False, both input keys and new keys generated by this\n",
      " |              chain will be returned. Defaults to False.\n",
      " |          callbacks: Callbacks to use for this chain run. These will be called in\n",
      " |              addition to callbacks passed to the chain during construction, but only\n",
      " |              these runtime callbacks will propagate to calls to other objects.\n",
      " |          tags: List of string tags to pass to all callbacks. These will be passed in\n",
      " |              addition to tags passed to the chain during construction, but only\n",
      " |              these runtime tags will propagate to calls to other objects.\n",
      " |          metadata: Optional metadata associated with the chain. Defaults to None\n",
      " |          include_run_info: Whether to include run info in the response. Defaults\n",
      " |              to False.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dict of named outputs. Should contain all outputs specified in\n",
      " |              `Chain.output_keys`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: langchain==0.1.0\n",
      " |         Use ainvoke instead.\n",
      " |  \n",
      " |  async ainvoke(self, input: Dict[str, Any], config: Optional[langchain_core.runnables.config.RunnableConfig] = None, **kwargs: Any) -> Dict[str, Any]\n",
      " |      Default implementation of ainvoke, calls invoke from a thread.\n",
      " |      \n",
      " |      The default implementation allows usage of async code even if\n",
      " |      the Runnable did not implement a native async version of invoke.\n",
      " |      \n",
      " |      Subclasses should override this method if they can run asynchronously.\n",
      " |  \n",
      " |  apply(self, input_list: List[Dict[str, Any]], callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None) -> List[Dict[str, str]]\n",
      " |      [*Deprecated*] Call the chain on all inputs in the list.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: langchain==0.1.0\n",
      " |         Use batch instead.\n",
      " |  \n",
      " |  async aprep_inputs(self, inputs: Union[Dict[str, Any], Any]) -> Dict[str, str]\n",
      " |      Prepare chain inputs, including adding inputs from memory.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Dictionary of raw inputs, or single input if chain expects\n",
      " |              only one param. Should contain all inputs specified in\n",
      " |              `Chain.input_keys` except for inputs that will be set by the chain's\n",
      " |              memory.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary of all inputs, including those added by the chain's memory.\n",
      " |  \n",
      " |  async aprep_outputs(self, inputs: Dict[str, str], outputs: Dict[str, str], return_only_outputs: bool = False) -> Dict[str, str]\n",
      " |      Validate and prepare chain outputs, and save info about this run to memory.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Dictionary of chain inputs, including any inputs added by chain\n",
      " |              memory.\n",
      " |          outputs: Dictionary of initial chain outputs.\n",
      " |          return_only_outputs: Whether to only return the chain outputs. If False,\n",
      " |              inputs are also added to the final outputs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dict of the final chain outputs.\n",
      " |  \n",
      " |  async arun(self, *args: Any, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, **kwargs: Any) -> Any\n",
      " |      [*Deprecated*] Convenience method for executing chain.\n",
      " |      \n",
      " |      The main difference between this method and `Chain.__call__` is that this\n",
      " |      method expects inputs to be passed directly in as positional arguments or\n",
      " |      keyword arguments, whereas `Chain.__call__` expects a single input dictionary\n",
      " |      with all the inputs\n",
      " |      \n",
      " |      \n",
      " |      Args:\n",
      " |          *args: If the chain expects a single input, it can be passed in as the\n",
      " |              sole positional argument.\n",
      " |          callbacks: Callbacks to use for this chain run. These will be called in\n",
      " |              addition to callbacks passed to the chain during construction, but only\n",
      " |              these runtime callbacks will propagate to calls to other objects.\n",
      " |          tags: List of string tags to pass to all callbacks. These will be passed in\n",
      " |              addition to tags passed to the chain during construction, but only\n",
      " |              these runtime tags will propagate to calls to other objects.\n",
      " |          **kwargs: If the chain expects multiple inputs, they can be passed in\n",
      " |              directly as keyword arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The chain output.\n",
      " |      \n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              # Suppose we have a single-input chain that takes a 'question' string:\n",
      " |              await chain.arun(\"What's the temperature in Boise, Idaho?\")\n",
      " |              # -> \"The temperature in Boise is...\"\n",
      " |      \n",
      " |              # Suppose we have a multi-input chain that takes a 'question' string\n",
      " |              # and 'context' string:\n",
      " |              question = \"What's the temperature in Boise, Idaho?\"\n",
      " |              context = \"Weather report for Boise, Idaho on 07/03/23...\"\n",
      " |              await chain.arun(question=question, context=context)\n",
      " |              # -> \"The temperature in Boise is...\"\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: langchain==0.1.0\n",
      " |         Use ainvoke instead.\n",
      " |  \n",
      " |  dict(self, **kwargs: Any) -> Dict\n",
      " |      Dictionary representation of chain.\n",
      " |      \n",
      " |      Expects `Chain._chain_type` property to be implemented and for memory to be\n",
      " |          null.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: Keyword arguments passed to default `pydantic.BaseModel.dict`\n",
      " |              method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary representation of the chain.\n",
      " |      \n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              chain.dict(exclude_unset=True)\n",
      " |              # -> {\"_type\": \"foo\", \"verbose\": False, ...}\n",
      " |  \n",
      " |  get_input_schema(self, config: Optional[langchain_core.runnables.config.RunnableConfig] = None) -> Type[pydantic.v1.main.BaseModel]\n",
      " |      Get a pydantic model that can be used to validate input to the Runnable.\n",
      " |      \n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic input schema that depends on which\n",
      " |      configuration the Runnable is invoked with.\n",
      " |      \n",
      " |      This method allows to get an input schema for a specific configuration.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate input.\n",
      " |  \n",
      " |  get_output_schema(self, config: Optional[langchain_core.runnables.config.RunnableConfig] = None) -> Type[pydantic.v1.main.BaseModel]\n",
      " |      Get a pydantic model that can be used to validate output to the Runnable.\n",
      " |      \n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic output schema that depends on which\n",
      " |      configuration the Runnable is invoked with.\n",
      " |      \n",
      " |      This method allows to get an output schema for a specific configuration.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate output.\n",
      " |  \n",
      " |  invoke(self, input: Dict[str, Any], config: Optional[langchain_core.runnables.config.RunnableConfig] = None, **kwargs: Any) -> Dict[str, Any]\n",
      " |      Transform a single input into an output. Override to implement.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: A config to use when invoking the Runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The output of the Runnable.\n",
      " |  \n",
      " |  prep_inputs(self, inputs: Union[Dict[str, Any], Any]) -> Dict[str, str]\n",
      " |      Prepare chain inputs, including adding inputs from memory.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Dictionary of raw inputs, or single input if chain expects\n",
      " |              only one param. Should contain all inputs specified in\n",
      " |              `Chain.input_keys` except for inputs that will be set by the chain's\n",
      " |              memory.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary of all inputs, including those added by the chain's memory.\n",
      " |  \n",
      " |  prep_outputs(self, inputs: Dict[str, str], outputs: Dict[str, str], return_only_outputs: bool = False) -> Dict[str, str]\n",
      " |      Validate and prepare chain outputs, and save info about this run to memory.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Dictionary of chain inputs, including any inputs added by chain\n",
      " |              memory.\n",
      " |          outputs: Dictionary of initial chain outputs.\n",
      " |          return_only_outputs: Whether to only return the chain outputs. If False,\n",
      " |              inputs are also added to the final outputs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dict of the final chain outputs.\n",
      " |  \n",
      " |  run(self, *args: Any, callbacks: Union[List[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[List[str]] = None, metadata: Optional[Dict[str, Any]] = None, **kwargs: Any) -> Any\n",
      " |      [*Deprecated*] Convenience method for executing chain.\n",
      " |      \n",
      " |      The main difference between this method and `Chain.__call__` is that this\n",
      " |      method expects inputs to be passed directly in as positional arguments or\n",
      " |      keyword arguments, whereas `Chain.__call__` expects a single input dictionary\n",
      " |      with all the inputs\n",
      " |      \n",
      " |      Args:\n",
      " |          *args: If the chain expects a single input, it can be passed in as the\n",
      " |              sole positional argument.\n",
      " |          callbacks: Callbacks to use for this chain run. These will be called in\n",
      " |              addition to callbacks passed to the chain during construction, but only\n",
      " |              these runtime callbacks will propagate to calls to other objects.\n",
      " |          tags: List of string tags to pass to all callbacks. These will be passed in\n",
      " |              addition to tags passed to the chain during construction, but only\n",
      " |              these runtime tags will propagate to calls to other objects.\n",
      " |          **kwargs: If the chain expects multiple inputs, they can be passed in\n",
      " |              directly as keyword arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The chain output.\n",
      " |      \n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              # Suppose we have a single-input chain that takes a 'question' string:\n",
      " |              chain.run(\"What's the temperature in Boise, Idaho?\")\n",
      " |              # -> \"The temperature in Boise is...\"\n",
      " |      \n",
      " |              # Suppose we have a multi-input chain that takes a 'question' string\n",
      " |              # and 'context' string:\n",
      " |              question = \"What's the temperature in Boise, Idaho?\"\n",
      " |              context = \"Weather report for Boise, Idaho on 07/03/23...\"\n",
      " |              chain.run(question=question, context=context)\n",
      " |              # -> \"The temperature in Boise is...\"\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: langchain==0.1.0\n",
      " |         Use invoke instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain.chains.base.Chain:\n",
      " |  \n",
      " |  raise_callback_manager_deprecation(values: Dict) -> Dict from pydantic.v1.main.ModelMetaclass\n",
      " |      Raise deprecation warning if callback_manager is used.\n",
      " |  \n",
      " |  set_verbose(verbose: Optional[bool]) -> bool from pydantic.v1.main.ModelMetaclass\n",
      " |      Set the chain verbosity.\n",
      " |      \n",
      " |      Defaults to the global setting if not specified by the user.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain.chains.base.Chain:\n",
      " |  \n",
      " |  Config = <class 'langchain.chains.base.Chain.Config'>\n",
      " |      Configuration for this pydantic object.\n",
      " |  \n",
      " |  \n",
      " |  __orig_bases__ = (langchain_core.runnables.base.RunnableSerializab...t...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |  \n",
      " |  configurable_alternatives(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]') -> 'RunnableSerializable[Input, Output]'\n",
      " |      Configure alternatives for Runnables that can be set at runtime.\n",
      " |      \n",
      " |      Args:\n",
      " |          which: The ConfigurableField instance that will be used to select the\n",
      " |              alternative.\n",
      " |          default_key: The default key to use if no alternative is selected.\n",
      " |              Defaults to \"default\".\n",
      " |          prefix_keys: Whether to prefix the keys with the ConfigurableField id.\n",
      " |              Defaults to False.\n",
      " |          **kwargs: A dictionary of keys to Runnable instances or callables that\n",
      " |              return Runnable instances.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable with the alternatives configured.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_anthropic import ChatAnthropic\n",
      " |          from langchain_core.runnables.utils import ConfigurableField\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |      \n",
      " |          model = ChatAnthropic(\n",
      " |              model_name=\"claude-3-sonnet-20240229\"\n",
      " |          ).configurable_alternatives(\n",
      " |              ConfigurableField(id=\"llm\"),\n",
      " |              default_key=\"anthropic\",\n",
      " |              openai=ChatOpenAI()\n",
      " |          )\n",
      " |      \n",
      " |          # uses the default model ChatAnthropic\n",
      " |          print(model.invoke(\"which organization created you?\").content)\n",
      " |      \n",
      " |          # uses ChatOpenAI\n",
      " |          print(\n",
      " |              model.with_config(\n",
      " |                  configurable={\"llm\": \"openai\"}\n",
      " |              ).invoke(\"which organization created you?\").content\n",
      " |          )\n",
      " |  \n",
      " |  configurable_fields(self, **kwargs: 'AnyConfigurableField') -> 'RunnableSerializable[Input, Output]'\n",
      " |      Configure particular Runnable fields at runtime.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: A dictionary of ConfigurableField instances to configure.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable with the fields configured.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import ConfigurableField\n",
      " |          from langchain_openai import ChatOpenAI\n",
      " |      \n",
      " |          model = ChatOpenAI(max_tokens=20).configurable_fields(\n",
      " |              max_tokens=ConfigurableField(\n",
      " |                  id=\"output_token_number\",\n",
      " |                  name=\"Max tokens in the output\",\n",
      " |                  description=\"The maximum number of tokens in the output\",\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |          # max_tokens = 20\n",
      " |          print(\n",
      " |              \"max_tokens_20: \",\n",
      " |              model.invoke(\"tell me something about chess\").content\n",
      " |          )\n",
      " |      \n",
      " |          # max_tokens = 200\n",
      " |          print(\"max_tokens_200: \", model.with_config(\n",
      " |              configurable={\"output_token_number\": 200}\n",
      " |              ).invoke(\"tell me something about chess\").content\n",
      " |          )\n",
      " |  \n",
      " |  to_json(self) -> 'Union[SerializedConstructor, SerializedNotImplemented]'\n",
      " |      Serialize the Runnable to JSON.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON-serializable representation of the Runnable.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.runnables.base.RunnableSerializable:\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  __repr_args__(self) -> Any\n",
      " |      Returns the attributes to show in __str__, __repr__, and __pretty__ this is generally overridden.\n",
      " |      \n",
      " |      Can either return:\n",
      " |      * name - value pairs, e.g.: `[('foo_name', 'foo'), ('bar_name', ['b', 'a', 'r'])]`\n",
      " |      * or, just values, e.g.: `[(None, 'foo'), (None, ['b', 'a', 'r'])]`\n",
      " |  \n",
      " |  to_json_not_implemented(self) -> langchain_core.load.serializable.SerializedNotImplemented\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  get_lc_namespace() -> List[str] from pydantic.v1.main.ModelMetaclass\n",
      " |      Get the namespace of the langchain object.\n",
      " |      \n",
      " |      For example, if the class is `langchain.llms.openai.OpenAI`, then the\n",
      " |      namespace is [\"langchain\", \"llms\", \"openai\"]\n",
      " |  \n",
      " |  is_lc_serializable() -> bool from pydantic.v1.main.ModelMetaclass\n",
      " |      Is this class serializable?\n",
      " |      \n",
      " |      By design, even if a class inherits from Serializable, it is not serializable by\n",
      " |      default. This is to prevent accidental serialization of objects that should not\n",
      " |      be serialized.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Whether the class is serializable. Default is False.\n",
      " |  \n",
      " |  lc_id() -> List[str] from pydantic.v1.main.ModelMetaclass\n",
      " |      A unique identifier for this class for serialization purposes.\n",
      " |      \n",
      " |      The unique identifier is a list of strings that describes the path\n",
      " |      to the object.\n",
      " |      For example, for the class `langchain.llms.openai.OpenAI`, the id is\n",
      " |      [\"langchain\", \"llms\", \"openai\", \"OpenAI\"].\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.load.serializable.Serializable:\n",
      " |  \n",
      " |  lc_attributes\n",
      " |      List of attribute names that should be included in the serialized kwargs.\n",
      " |      \n",
      " |      These attributes must be accepted by the constructor.\n",
      " |      Default is an empty dictionary.\n",
      " |  \n",
      " |  lc_secrets\n",
      " |      A map of constructor argument names to secret ids.\n",
      " |      \n",
      " |      For example,\n",
      " |          {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.main.BaseModel:\n",
      " |  \n",
      " |  __eq__(self, other: Any) -> bool\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __getstate__(self) -> 'DictAny'\n",
      " |  \n",
      " |  __init__(__pydantic_self__, **data: Any) -> None\n",
      " |      Create a new model by parsing and validating input data from keyword arguments.\n",
      " |      \n",
      " |      Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
      " |  \n",
      " |  __iter__(self) -> 'TupleGenerator'\n",
      " |      so `dict(model)` works\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state: 'DictAny') -> None\n",
      " |  \n",
      " |  copy(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -> 'Model'\n",
      " |      Duplicate a model, optionally choose which fields to include, exclude and change.\n",
      " |      \n",
      " |      :param include: fields to include in new model\n",
      " |      :param exclude: fields to exclude from new model, as with values this takes precedence over include\n",
      " |      :param update: values to change/add in the new model. Note: the data is not validated before creating\n",
      " |          the new model: you should trust this data\n",
      " |      :param deep: set to `True` to make a deep copy of the model\n",
      " |      :return: new model instance\n",
      " |  \n",
      " |  json(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -> str\n",
      " |      Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.\n",
      " |      \n",
      " |      `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pydantic.v1.main.BaseModel:\n",
      " |  \n",
      " |  __get_validators__() -> 'CallableGenerator' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  __try_update_forward_refs__(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      " |      Same as update_forward_refs but will not raise exception\n",
      " |      when forward references are not defined.\n",
      " |  \n",
      " |  construct(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |      Creates a new model setting __dict__ and __fields_set__ from trusted or pre-validated data.\n",
      " |      Default values are respected, but no other validation is performed.\n",
      " |      Behaves as if `Config.extra = 'allow'` was set since it adds all passed values\n",
      " |  \n",
      " |  from_orm(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_file(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_obj(obj: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  parse_raw(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  schema(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -> 'DictStrAny' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  schema_json(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -> str from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  update_forward_refs(**localns: Any) -> None from pydantic.v1.main.ModelMetaclass\n",
      " |      Try to update ForwardRefs on fields based on this Model, globalns and localns.\n",
      " |  \n",
      " |  validate(value: Any) -> 'Model' from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pydantic.v1.main.BaseModel:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __fields_set__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pydantic.v1.utils.Representation:\n",
      " |  \n",
      " |  __pretty__(self, fmt: Callable[[Any], Any], **kwargs: Any) -> Generator[Any, NoneType, NoneType]\n",
      " |      Used by devtools (https://python-devtools.helpmanual.io/) to provide a human readable representations of objects\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __repr_name__(self) -> str\n",
      " |      Name of the instance's class, used in __repr__.\n",
      " |  \n",
      " |  __repr_str__(self, join_str: str) -> str\n",
      " |  \n",
      " |  __rich_repr__(self) -> 'RichReprResult'\n",
      " |      Get fields for Rich library\n",
      " |  \n",
      " |  __str__(self) -> str\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Callable[[Iterator[Any]], Iterator[Other]], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this Runnable with another object to create a RunnableSequence.\n",
      " |  \n",
      " |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Other], Any], Callable[[Iterator[Other]], Iterator[Any]], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSerializable[Other, Output]'\n",
      " |      Compose this Runnable with another object to create a RunnableSequence.\n",
      " |  \n",
      " |  async abatch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
      " |      Default implementation runs ainvoke in parallel using asyncio.gather.\n",
      " |      \n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |      \n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying Runnable uses an API which supports a batch mode.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the Runnable.\n",
      " |          config: A config to use when invoking the Runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details. Defaults to None.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |              Defaults to False.\n",
      " |          **kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of outputs from the Runnable.\n",
      " |  \n",
      " |  async abatch_as_completed(self, inputs: 'Sequence[Input]', config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Tuple[int, Union[Output, Exception]]]'\n",
      " |      Run ainvoke in parallel on a list of inputs,\n",
      " |      yielding results as they complete.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the Runnable.\n",
      " |          config: A config to use when invoking the Runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details. Defaults to None. Defaults to None.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |              Defaults to False.\n",
      " |          **kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |      \n",
      " |      Yields:\n",
      " |          A tuple of the index of the input and the output from the Runnable.\n",
      " |  \n",
      " |  as_tool(self, args_schema: 'Optional[Type[BaseModel]]' = None, *, name: 'Optional[str]' = None, description: 'Optional[str]' = None, arg_types: 'Optional[Dict[str, Type]]' = None) -> 'BaseTool'\n",
      " |      [*Beta*] Create a BaseTool from a Runnable.\n",
      " |      \n",
      " |      ``as_tool`` will instantiate a BaseTool with a name, description, and\n",
      " |      ``args_schema`` from a Runnable. Where possible, schemas are inferred\n",
      " |      from ``runnable.get_input_schema``. Alternatively (e.g., if the\n",
      " |      Runnable takes a dict as input and the specific dict keys are not typed),\n",
      " |      the schema can be specified directly with ``args_schema``. You can also\n",
      " |      pass ``arg_types`` to just specify the required arguments and their types.\n",
      " |      \n",
      " |      Args:\n",
      " |          args_schema: The schema for the tool. Defaults to None.\n",
      " |          name: The name of the tool. Defaults to None.\n",
      " |          description: The description of the tool. Defaults to None.\n",
      " |          arg_types: A dictionary of argument names to types. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A BaseTool instance.\n",
      " |      \n",
      " |      Typed dict input:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from typing import List\n",
      " |          from typing_extensions import TypedDict\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          class Args(TypedDict):\n",
      " |              a: int\n",
      " |              b: List[int]\n",
      " |      \n",
      " |          def f(x: Args) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |      \n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool()\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |      \n",
      " |      ``dict`` input, specifying schema via ``args_schema``:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from typing import Any, Dict, List\n",
      " |          from langchain_core.pydantic_v1 import BaseModel, Field\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          def f(x: Dict[str, Any]) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |      \n",
      " |          class FSchema(BaseModel):\n",
      " |              \"\"\"Apply a function to an integer and list of integers.\"\"\"\n",
      " |      \n",
      " |              a: int = Field(..., description=\"Integer\")\n",
      " |              b: List[int] = Field(..., description=\"List of ints\")\n",
      " |      \n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool(FSchema)\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |      \n",
      " |      ``dict`` input, specifying schema via ``arg_types``:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from typing import Any, Dict, List\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          def f(x: Dict[str, Any]) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |      \n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool(arg_types={\"a\": int, \"b\": List[int]})\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |      \n",
      " |      String input:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          def f(x: str) -> str:\n",
      " |              return x + \"a\"\n",
      " |      \n",
      " |          def g(x: str) -> str:\n",
      " |              return x + \"z\"\n",
      " |      \n",
      " |          runnable = RunnableLambda(f) | g\n",
      " |          as_tool = runnable.as_tool()\n",
      " |          as_tool.invoke(\"b\")\n",
      " |      \n",
      " |      .. versionadded:: 0.2.14\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. beta::\n",
      " |         This API is in beta and may change in the future.\n",
      " |  \n",
      " |  assign(self, **kwargs: 'Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any], Mapping[str, Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]]]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Assigns new fields to the dict output of this Runnable.\n",
      " |      Returns a new Runnable.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_community.llms.fake import FakeStreamingListLLM\n",
      " |          from langchain_core.output_parsers import StrOutputParser\n",
      " |          from langchain_core.prompts import SystemMessagePromptTemplate\n",
      " |          from langchain_core.runnables import Runnable\n",
      " |          from operator import itemgetter\n",
      " |      \n",
      " |          prompt = (\n",
      " |              SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n",
      " |              + \"{question}\"\n",
      " |          )\n",
      " |          llm = FakeStreamingListLLM(responses=[\"foo-lish\"])\n",
      " |      \n",
      " |          chain: Runnable = prompt | llm | {\"str\": StrOutputParser()}\n",
      " |      \n",
      " |          chain_with_assign = chain.assign(hello=itemgetter(\"str\") | llm)\n",
      " |      \n",
      " |          print(chain_with_assign.input_schema.schema())\n",
      " |          # {'title': 'PromptInput', 'type': 'object', 'properties':\n",
      " |          {'question': {'title': 'Question', 'type': 'string'}}}\n",
      " |          print(chain_with_assign.output_schema.schema()) #\n",
      " |          {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n",
      " |          {'str': {'title': 'Str',\n",
      " |          'type': 'string'}, 'hello': {'title': 'Hello', 'type': 'string'}}}\n",
      " |  \n",
      " |  astream_events(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, version: \"Literal['v1', 'v2']\", include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[StreamEvent]'\n",
      " |      [*Beta*] Generate a stream of events.\n",
      " |      \n",
      " |      Use to create an iterator over StreamEvents that provide real-time information\n",
      " |      about the progress of the Runnable, including StreamEvents from intermediate\n",
      " |      results.\n",
      " |      \n",
      " |      A StreamEvent is a dictionary with the following schema:\n",
      " |      \n",
      " |      - ``event``: **str** - Event names are of the\n",
      " |          format: on_[runnable_type]_(start|stream|end).\n",
      " |      - ``name``: **str** - The name of the Runnable that generated the event.\n",
      " |      - ``run_id``: **str** - randomly generated ID associated with the given execution of\n",
      " |          the Runnable that emitted the event.\n",
      " |          A child Runnable that gets invoked as part of the execution of a\n",
      " |          parent Runnable is assigned its own unique ID.\n",
      " |      - ``parent_ids``: **List[str]** - The IDs of the parent runnables that\n",
      " |          generated the event. The root Runnable will have an empty list.\n",
      " |          The order of the parent IDs is from the root to the immediate parent.\n",
      " |          Only available for v2 version of the API. The v1 version of the API\n",
      " |          will return an empty list.\n",
      " |      - ``tags``: **Optional[List[str]]** - The tags of the Runnable that generated\n",
      " |          the event.\n",
      " |      - ``metadata``: **Optional[Dict[str, Any]]** - The metadata of the Runnable\n",
      " |          that generated the event.\n",
      " |      - ``data``: **Dict[str, Any]**\n",
      " |      \n",
      " |      \n",
      " |      Below is a table that illustrates some evens that might be emitted by various\n",
      " |      chains. Metadata fields have been omitted from the table for brevity.\n",
      " |      Chain definitions have been included after the table.\n",
      " |      \n",
      " |      **ATTENTION** This reference table is for the V2 version of the schema.\n",
      " |      \n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | event                | name             | chunk                           | input                                         | output                                          |\n",
      " |      +======================+==================+=================================+===============================================+=================================================+\n",
      " |      | on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | AIMessageChunk(content=\"hello world\")           |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_end           | [model name]     |                                 | 'Hello human!'                                |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_start       | format_docs      |                                 |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | [Document(...), ..]                             |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      \n",
      " |      In addition to the standard events, users can also dispatch custom events (see example below).\n",
      " |      \n",
      " |      Custom events will be only be surfaced with in the `v2` version of the API!\n",
      " |      \n",
      " |      A custom event has following format:\n",
      " |      \n",
      " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
      " |      | Attribute | Type | Description                                                                                               |\n",
      " |      +===========+======+===========================================================================================================+\n",
      " |      | name      | str  | A user defined name for the event.                                                                        |\n",
      " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
      " |      | data      | Any  | The data associated with the event. This can be anything, though we suggest making it JSON serializable.  |\n",
      " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
      " |      \n",
      " |      Here are declarations associated with the standard events shown above:\n",
      " |      \n",
      " |      `format_docs`:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          def format_docs(docs: List[Document]) -> str:\n",
      " |              '''Format the docs.'''\n",
      " |              return \", \".join([doc.page_content for doc in docs])\n",
      " |      \n",
      " |          format_docs = RunnableLambda(format_docs)\n",
      " |      \n",
      " |      `some_tool`:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          @tool\n",
      " |          def some_tool(x: int, y: str) -> dict:\n",
      " |              '''Some_tool.'''\n",
      " |              return {\"x\": x, \"y\": y}\n",
      " |      \n",
      " |      `prompt`:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          template = ChatPromptTemplate.from_messages(\n",
      " |              [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n",
      " |          ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
      " |      \n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          async def reverse(s: str) -> str:\n",
      " |              return s[::-1]\n",
      " |      \n",
      " |          chain = RunnableLambda(func=reverse)\n",
      " |      \n",
      " |          events = [\n",
      " |              event async for event in chain.astream_events(\"hello\", version=\"v2\")\n",
      " |          ]\n",
      " |      \n",
      " |          # will produce the following events (run_id, and parent_ids\n",
      " |          # has been omitted for brevity):\n",
      " |          [\n",
      " |              {\n",
      " |                  \"data\": {\"input\": \"hello\"},\n",
      " |                  \"event\": \"on_chain_start\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"chunk\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_stream\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"output\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_end\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |          ]\n",
      " |      \n",
      " |      \n",
      " |      Example: Dispatch Custom Event\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.callbacks.manager import (\n",
      " |              adispatch_custom_event,\n",
      " |          )\n",
      " |          from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
      " |          import asyncio\n",
      " |      \n",
      " |      \n",
      " |          async def slow_thing(some_input: str, config: RunnableConfig) -> str:\n",
      " |              \"\"\"Do something that takes a long time.\"\"\"\n",
      " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |              await adispatch_custom_event(\n",
      " |                  \"progress_event\",\n",
      " |                  {\"message\": \"Finished step 1 of 3\"},\n",
      " |                  config=config # Must be included for python < 3.10\n",
      " |              )\n",
      " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |              await adispatch_custom_event(\n",
      " |                  \"progress_event\",\n",
      " |                  {\"message\": \"Finished step 2 of 3\"},\n",
      " |                  config=config # Must be included for python < 3.10\n",
      " |              )\n",
      " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |              return \"Done\"\n",
      " |      \n",
      " |          slow_thing = RunnableLambda(slow_thing)\n",
      " |      \n",
      " |          async for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\n",
      " |              print(event)\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: The config to use for the Runnable.\n",
      " |          version: The version of the schema to use either `v2` or `v1`.\n",
      " |                   Users should use `v2`.\n",
      " |                   `v1` is for backwards compatibility and will be deprecated\n",
      " |                   in 0.4.0.\n",
      " |                   No default will be assigned until the API is stabilized.\n",
      " |                   custom events will only be surfaced in `v2`.\n",
      " |          include_names: Only include events from runnables with matching names.\n",
      " |          include_types: Only include events from runnables with matching types.\n",
      " |          include_tags: Only include events from runnables with matching tags.\n",
      " |          exclude_names: Exclude events from runnables with matching names.\n",
      " |          exclude_types: Exclude events from runnables with matching types.\n",
      " |          exclude_tags: Exclude events from runnables with matching tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |              These will be passed to astream_log as this implementation\n",
      " |              of astream_events is built on top of astream_log.\n",
      " |      \n",
      " |      Yields:\n",
      " |          An async stream of StreamEvents.\n",
      " |      \n",
      " |      Raises:\n",
      " |          NotImplementedError: If the version is not `v1` or `v2`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. beta::\n",
      " |         This API is in beta and may change in the future.\n",
      " |  \n",
      " |  async astream_log(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'\n",
      " |      Stream all output from a Runnable, as reported to the callback system.\n",
      " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
      " |      \n",
      " |      Output is streamed as Log objects, which include a list of\n",
      " |      Jsonpatch ops that describe how the state of the run has changed in each\n",
      " |      step, and the final state of the run.\n",
      " |      \n",
      " |      The Jsonpatch ops can be applied in order to construct state.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: The config to use for the Runnable.\n",
      " |          diff: Whether to yield diffs between each step or the current state.\n",
      " |          with_streamed_output_list: Whether to yield the streamed_output list.\n",
      " |          include_names: Only include logs with these names.\n",
      " |          include_types: Only include logs with these types.\n",
      " |          include_tags: Only include logs with these tags.\n",
      " |          exclude_names: Exclude logs with these names.\n",
      " |          exclude_types: Exclude logs with these types.\n",
      " |          exclude_tags: Exclude logs with these tags.\n",
      " |          **kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |      \n",
      " |      Yields:\n",
      " |          A RunLogPatch or RunLog object.\n",
      " |  \n",
      " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Default implementation of atransform, which buffers input and calls astream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: An async iterator of inputs to the Runnable.\n",
      " |          config: The config to use for the Runnable. Defaults to None.\n",
      " |          **kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |      \n",
      " |      Yields:\n",
      " |          The output of the Runnable.\n",
      " |  \n",
      " |  batch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
      " |      Default implementation runs invoke in parallel using a thread pool executor.\n",
      " |      \n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |      \n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying Runnable uses an API which supports a batch mode.\n",
      " |  \n",
      " |  batch_as_completed(self, inputs: 'Sequence[Input]', config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'Iterator[Tuple[int, Union[Output, Exception]]]'\n",
      " |      Run invoke in parallel on a list of inputs,\n",
      " |      yielding results as they complete.\n",
      " |  \n",
      " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind arguments to a Runnable, returning a new Runnable.\n",
      " |      \n",
      " |      Useful when a Runnable in a chain requires an argument that is not\n",
      " |      in the output of the previous Runnable or included in the user input.\n",
      " |      \n",
      " |      Args:\n",
      " |          kwargs: The arguments to bind to the Runnable.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable with the arguments bound.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_community.chat_models import ChatOllama\n",
      " |          from langchain_core.output_parsers import StrOutputParser\n",
      " |      \n",
      " |          llm = ChatOllama(model='llama2')\n",
      " |      \n",
      " |          # Without bind.\n",
      " |          chain = (\n",
      " |              llm\n",
      " |              | StrOutputParser()\n",
      " |          )\n",
      " |      \n",
      " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      " |          # Output is 'One two three four five.'\n",
      " |      \n",
      " |          # With bind.\n",
      " |          chain = (\n",
      " |              llm.bind(stop=[\"three\"])\n",
      " |              | StrOutputParser()\n",
      " |          )\n",
      " |      \n",
      " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      " |          # Output is 'One two'\n",
      " |  \n",
      " |  config_schema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'Type[BaseModel]'\n",
      " |      The type of config this Runnable accepts specified as a pydantic model.\n",
      " |      \n",
      " |      To mark a field as configurable, see the `configurable_fields`\n",
      " |      and `configurable_alternatives` methods.\n",
      " |      \n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate config.\n",
      " |  \n",
      " |  get_graph(self, config: 'Optional[RunnableConfig]' = None) -> 'Graph'\n",
      " |      Return a graph representation of this Runnable.\n",
      " |  \n",
      " |  get_name(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -> 'str'\n",
      " |      Get the name of the Runnable.\n",
      " |  \n",
      " |  get_prompts(self, config: 'Optional[RunnableConfig]' = None) -> 'List[BasePromptTemplate]'\n",
      " |      Return a list of prompts used by this Runnable.\n",
      " |  \n",
      " |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      " |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      " |      by calling invoke() with each input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that maps a list of inputs to a list of outputs.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |                  from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |                  def _lambda(x: int) -> int:\n",
      " |                      return x + 1\n",
      " |      \n",
      " |                  runnable = RunnableLambda(_lambda)\n",
      " |                  print(runnable.map().invoke([1, 2, 3])) # [2, 3, 4]\n",
      " |  \n",
      " |  pick(self, keys: 'Union[str, List[str]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Pick keys from the dict output of this Runnable.\n",
      " |      \n",
      " |      Pick single key:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              import json\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
      " |      \n",
      " |              as_str = RunnableLambda(str)\n",
      " |              as_json = RunnableLambda(json.loads)\n",
      " |              chain = RunnableMap(str=as_str, json=as_json)\n",
      " |      \n",
      " |              chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\n",
      " |      \n",
      " |              json_only_chain = chain.pick(\"json\")\n",
      " |              json_only_chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> [1, 2, 3]\n",
      " |      \n",
      " |      Pick list of keys:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from typing import Any\n",
      " |      \n",
      " |              import json\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
      " |      \n",
      " |              as_str = RunnableLambda(str)\n",
      " |              as_json = RunnableLambda(json.loads)\n",
      " |              def as_bytes(x: Any) -> bytes:\n",
      " |                  return bytes(x, \"utf-8\")\n",
      " |      \n",
      " |              chain = RunnableMap(\n",
      " |                  str=as_str,\n",
      " |                  json=as_json,\n",
      " |                  bytes=RunnableLambda(as_bytes)\n",
      " |              )\n",
      " |      \n",
      " |              chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
      " |      \n",
      " |              json_and_bytes_chain = chain.pick([\"json\", \"bytes\"])\n",
      " |              json_and_bytes_chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
      " |  \n",
      " |  pipe(self, *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]', name: 'Optional[str]' = None) -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this Runnable with Runnable-like objects to make a RunnableSequence.\n",
      " |      \n",
      " |      Equivalent to `RunnableSequence(self, *others)` or `self | others[0] | ...`\n",
      " |      \n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |              def add_one(x: int) -> int:\n",
      " |                  return x + 1\n",
      " |      \n",
      " |              def mul_two(x: int) -> int:\n",
      " |                  return x * 2\n",
      " |      \n",
      " |              runnable_1 = RunnableLambda(add_one)\n",
      " |              runnable_2 = RunnableLambda(mul_two)\n",
      " |              sequence = runnable_1.pipe(runnable_2)\n",
      " |              # Or equivalently:\n",
      " |              # sequence = runnable_1 | runnable_2\n",
      " |              # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
      " |              sequence.invoke(1)\n",
      " |              await sequence.ainvoke(1)\n",
      " |              # -> 4\n",
      " |      \n",
      " |              sequence.batch([1, 2, 3])\n",
      " |              await sequence.abatch([1, 2, 3])\n",
      " |              # -> [4, 6, 8]\n",
      " |  \n",
      " |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Default implementation of transform, which buffers input and then calls stream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |      \n",
      " |      Args:\n",
      " |          input: An iterator of inputs to the Runnable.\n",
      " |          config: The config to use for the Runnable. Defaults to None.\n",
      " |          **kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |      \n",
      " |      Yields:\n",
      " |          The output of the Runnable.\n",
      " |  \n",
      " |  with_alisteners(self, *, on_start: 'Optional[AsyncListener]' = None, on_end: 'Optional[AsyncListener]' = None, on_error: 'Optional[AsyncListener]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind asynchronous lifecycle listeners to a Runnable, returning a new Runnable.\n",
      " |      \n",
      " |      on_start: Asynchronously called before the Runnable starts running.\n",
      " |      on_end: Asynchronously called after the Runnable finishes running.\n",
      " |      on_error: Asynchronously called if the Runnable throws an error.\n",
      " |      \n",
      " |      The Run object contains information about the run, including its id,\n",
      " |      type, input, output, error, start_time, end_time, and any tags or metadata\n",
      " |      added to the run.\n",
      " |      \n",
      " |      Args:\n",
      " |          on_start: Asynchronously called before the Runnable starts running.\n",
      " |              Defaults to None.\n",
      " |          on_end: Asynchronously called after the Runnable finishes running.\n",
      " |              Defaults to None.\n",
      " |          on_error: Asynchronously called if the Runnable throws an error.\n",
      " |              Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable with the listeners bound.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |          import time\n",
      " |      \n",
      " |          async def test_runnable(time_to_sleep : int):\n",
      " |              print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\n",
      " |              await asyncio.sleep(time_to_sleep)\n",
      " |              print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\n",
      " |      \n",
      " |          async def fn_start(run_obj : Runnable):\n",
      " |              print(f\"on start callback starts at {format_t(time.time())}\n",
      " |              await asyncio.sleep(3)\n",
      " |              print(f\"on start callback ends at {format_t(time.time())}\")\n",
      " |      \n",
      " |          async def fn_end(run_obj : Runnable):\n",
      " |              print(f\"on end callback starts at {format_t(time.time())}\n",
      " |              await asyncio.sleep(2)\n",
      " |              print(f\"on end callback ends at {format_t(time.time())}\")\n",
      " |      \n",
      " |          runnable = RunnableLambda(test_runnable).with_alisteners(\n",
      " |              on_start=fn_start,\n",
      " |              on_end=fn_end\n",
      " |          )\n",
      " |          async def concurrent_runs():\n",
      " |              await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\n",
      " |      \n",
      " |          asyncio.run(concurrent_runs())\n",
      " |          Result:\n",
      " |          on start callback starts at 2024-05-16T14:20:29.637053+00:00\n",
      " |          on start callback starts at 2024-05-16T14:20:29.637150+00:00\n",
      " |          on start callback ends at 2024-05-16T14:20:32.638305+00:00\n",
      " |          on start callback ends at 2024-05-16T14:20:32.638383+00:00\n",
      " |          Runnable[3s]: starts at 2024-05-16T14:20:32.638849+00:00\n",
      " |          Runnable[5s]: starts at 2024-05-16T14:20:32.638999+00:00\n",
      " |          Runnable[3s]: ends at 2024-05-16T14:20:35.640016+00:00\n",
      " |          on end callback starts at 2024-05-16T14:20:35.640534+00:00\n",
      " |          Runnable[5s]: ends at 2024-05-16T14:20:37.640169+00:00\n",
      " |          on end callback starts at 2024-05-16T14:20:37.640574+00:00\n",
      " |          on end callback ends at 2024-05-16T14:20:37.640654+00:00\n",
      " |          on end callback ends at 2024-05-16T14:20:39.641751+00:00\n",
      " |  \n",
      " |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind config to a Runnable, returning a new Runnable.\n",
      " |      \n",
      " |      Args:\n",
      " |          config: The config to bind to the Runnable.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable with the config bound.\n",
      " |  \n",
      " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), exception_key: 'Optional[str]' = None) -> 'RunnableWithFallbacksT[Input, Output]'\n",
      " |      Add fallbacks to a Runnable, returning a new Runnable.\n",
      " |      \n",
      " |      The new Runnable will try the original Runnable, and then each fallback\n",
      " |      in order, upon failures.\n",
      " |      \n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original Runnable fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |              Defaults to (Exception,).\n",
      " |          exception_key: If string is specified then handled exceptions will be passed\n",
      " |              to fallbacks as part of the input under the specified key. If None,\n",
      " |              exceptions will not be passed to fallbacks. If used, the base Runnable\n",
      " |              and its fallbacks must accept a dictionary as input. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that will try the original Runnable, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              from typing import Iterator\n",
      " |      \n",
      " |              from langchain_core.runnables import RunnableGenerator\n",
      " |      \n",
      " |      \n",
      " |              def _generate_immediate_error(input: Iterator) -> Iterator[str]:\n",
      " |                  raise ValueError()\n",
      " |                  yield \"\"\n",
      " |      \n",
      " |      \n",
      " |              def _generate(input: Iterator) -> Iterator[str]:\n",
      " |                  yield from \"foo bar\"\n",
      " |      \n",
      " |      \n",
      " |              runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\n",
      " |                  [RunnableGenerator(_generate)]\n",
      " |                  )\n",
      " |              print(''.join(runnable.stream({}))) #foo bar\n",
      " |      \n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original Runnable fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |          exception_key: If string is specified then handled exceptions will be passed\n",
      " |              to fallbacks as part of the input under the specified key. If None,\n",
      " |              exceptions will not be passed to fallbacks. If used, the base Runnable\n",
      " |              and its fallbacks must accept a dictionary as input.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that will try the original Runnable, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |  \n",
      " |  with_listeners(self, *, on_start: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None, on_end: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None, on_error: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind lifecycle listeners to a Runnable, returning a new Runnable.\n",
      " |      \n",
      " |      on_start: Called before the Runnable starts running, with the Run object.\n",
      " |      on_end: Called after the Runnable finishes running, with the Run object.\n",
      " |      on_error: Called if the Runnable throws an error, with the Run object.\n",
      " |      \n",
      " |      The Run object contains information about the run, including its id,\n",
      " |      type, input, output, error, start_time, end_time, and any tags or metadata\n",
      " |      added to the run.\n",
      " |      \n",
      " |      Args:\n",
      " |          on_start: Called before the Runnable starts running. Defaults to None.\n",
      " |          on_end: Called after the Runnable finishes running. Defaults to None.\n",
      " |          on_error: Called if the Runnable throws an error. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable with the listeners bound.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |          from langchain_core.tracers.schemas import Run\n",
      " |      \n",
      " |          import time\n",
      " |      \n",
      " |          def test_runnable(time_to_sleep : int):\n",
      " |              time.sleep(time_to_sleep)\n",
      " |      \n",
      " |          def fn_start(run_obj: Run):\n",
      " |              print(\"start_time:\", run_obj.start_time)\n",
      " |      \n",
      " |          def fn_end(run_obj: Run):\n",
      " |              print(\"end_time:\", run_obj.end_time)\n",
      " |      \n",
      " |          chain = RunnableLambda(test_runnable).with_listeners(\n",
      " |              on_start=fn_start,\n",
      " |              on_end=fn_end\n",
      " |          )\n",
      " |          chain.invoke(2)\n",
      " |  \n",
      " |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      " |      Create a new Runnable that retries the original Runnable on exceptions.\n",
      " |      \n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on.\n",
      " |              Defaults to (Exception,).\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait\n",
      " |              time between retries. Defaults to True.\n",
      " |          stop_after_attempt: The maximum number of attempts to make before\n",
      " |              giving up. Defaults to 3.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that retries the original Runnable on exceptions.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |      \n",
      " |          count = 0\n",
      " |      \n",
      " |      \n",
      " |          def _lambda(x: int) -> None:\n",
      " |              global count\n",
      " |              count = count + 1\n",
      " |              if x == 1:\n",
      " |                  raise ValueError(\"x is 1\")\n",
      " |              else:\n",
      " |                   pass\n",
      " |      \n",
      " |      \n",
      " |          runnable = RunnableLambda(_lambda)\n",
      " |          try:\n",
      " |              runnable.with_retry(\n",
      " |                  stop_after_attempt=2,\n",
      " |                  retry_if_exception_type=(ValueError,),\n",
      " |              ).invoke(1)\n",
      " |          except ValueError:\n",
      " |              pass\n",
      " |      \n",
      " |          assert (count == 2)\n",
      " |      \n",
      " |      \n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait time\n",
      " |                                   between retries\n",
      " |          stop_after_attempt: The maximum number of attempts to make before giving up\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable that retries the original Runnable on exceptions.\n",
      " |  \n",
      " |  with_types(self, *, input_type: 'Optional[Type[Input]]' = None, output_type: 'Optional[Type[Output]]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind input and output types to a Runnable, returning a new Runnable.\n",
      " |      \n",
      " |      Args:\n",
      " |          input_type: The input type to bind to the Runnable. Defaults to None.\n",
      " |          output_type: The output type to bind to the Runnable. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A new Runnable with the types bound.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  InputType\n",
      " |      The type of input this Runnable accepts specified as a type annotation.\n",
      " |  \n",
      " |  OutputType\n",
      " |      The type of output this Runnable produces specified as a type annotation.\n",
      " |  \n",
      " |  config_specs\n",
      " |      List configurable fields for this Runnable.\n",
      " |  \n",
      " |  input_schema\n",
      " |      The type of input this Runnable accepts specified as a pydantic model.\n",
      " |  \n",
      " |  output_schema\n",
      " |      The type of output this Runnable produces specified as a pydantic model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain_core.runnables.base.Runnable:\n",
      " |  \n",
      " |  name = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from pydantic.v1.main.ModelMetaclass\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from pydantic.v1.main.ModelMetaclass\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(AgentExecutor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614794f8-f99c-447b-8fd3-8c2cdd8a0517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82cf68b-1127-4318-8996-b3615302e948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713555a7-5799-4a6c-9b1e-1c18fe46d001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
